# Regularization, Bayes and Shrinkage
本节主要内容来自于机器学习与资产定价（2020，Nagel著，王熙、石川译）。

## 从Regularization谈起
### Factor Zoo and Regularization
一直以来，资产定价方面的研究都是通过<mark>关注低维模型来回避资产价格预测变量的高维度问题</mark>。例如，在研究股票截面收益率预测时，学者们仅仅在回归中使用少量公司特征作为解释变量，他们希望使用仅包含少数几个因子的多因子模型来捕捉股票收益率截面上的投资机会。

鉴于为数众多的变量都可能预测股票收益率以及被拿来构造基于公司特征的因子投资组合，在研究中仅仅关注很少数量的因子表明学者们在模型中强加了很强的**稀疏性（Sparsity）**，这意味着将其余成百上千的因子对收益率的影响视为0，这一设定很大程度上取决于**研究者的主观研究兴趣以及对于数据的贴合程度**，因此，也被称之为特设稀疏性假设（ad hoc Sparsity）。

在模型中施加如此极端的稀疏性约束能够确保传统的统计方法具有良好的表现，然而这背后并没有令人信服的经济学理论依据。后续的因子研究表明，似乎成百上千个因子都具有预测能力。针对于这一问题，Cochrane(2011)在AFA的president address中提出了factor zoo的概念，并问到究竟有多少个因子真正具有预测效力？

如今学术研究正在逐步意识到并慢慢适应先前研究中确实存在遗漏因子的问题，传统的因子研究范式从三因子扩充到四因子、五因子。当被纳入的因子变量越来越多，预测变量高维度的背后也隐含着一个统计问题。当预测股票收益率的变量个数（J）接近或超过能够观测到的股票数量（N）时，常规的统计学方法（例如普通最小二乘法（OLS））并不适用。当数据量并不显著大于预测变量时，OLS回归会过度拟合数据中的噪声，导致样本内和样本外表现差异巨大。

然而，***通过贝叶斯方法，过拟合问题得以规避***。因为贝叶斯能够自动选取最合适且有效的参数数量。

## Bayes Interpretation

> [!NOTE|label:注意]
> 正则化本身具有贝叶斯解释，而这种解释可以被理解为对模型参数施加了某些先验分布。Kozak,Nagel and Santosh(2020)正是在这种贝叶斯解释基础上，将经济学推理和先验知识注入针对资产定价的机器学习方法设计中。


### 两种正则化方法

在线性回归模型中:
$$
y_i = x_i'g+\epsilon_i
$$
其中$g$代表未知回归系数向量。假设有N个观测值,将其堆叠成$N\times 1$为向量$y = (y_1,y_2,...y_N)'$ 和$N\times K$维矩阵$X=(x_1,x_2....,x_N')$。估计 $g$ 的常用方法为使得误差平方最小,即目标函数为:
$$
\mathop{min}\limits_{g} (y-Xg)'(y-Xg)
$$

将目标函数对$g$ 进行微分，令一阶导数为0并求解 $g$ ，便得到OLS估计量：
$$
\hat{g} = (X'X)^{-1}X'y
$$
以及样本内拟合值：
$$
\hat{y} = X\hat{g}
$$

当变量个数K过大时，可以通过正则项 $\hat{g}$ 来避免其中某些元素的取值幅度过高从而能够改善预测性能。接下里介绍两种本节中经常提及的正则化方法。

#### Ridge
在岭回归（Hoerl and Kennard 1970）中，优化的目标在最小化与OLS相同的误差平方和损失函数基础上，补充了$L^2$ 正则项 $g'g$：
$$
\mathop{min}\limits_{g} \Big[ {1\over N}(y-Xg)'(y-Xg) +\gamma g'g]
$$
因此，目标函数包括两个部分。第一项代表损失（loss），也就是基础的OLS项。第二项为正则项，其中超参数 $\gamma$ 控制正则的强度，其解为：
$$
\hat{g}=(X'X+\gamma I_k)^{-1} X'y
$$
其中 $I_k$ 是 $K \times K$ 维单位矩阵。通过向 $X'X$ 添加对角矩阵，即<mark>岭</mark>。求逆运算时，$\gamma I_k$ 的存在将导致回归系数向0收缩。直观的说，正则项 $g'g$ 的存在会使得 $\hat{g}$ 中取值幅度过高的元素进行惩罚，因此相比于OLS而言系数更接近0。

**当 $X$ 为正交矩阵**，即 $X'X = I_K$，则有：
$$
\hat{g} = {\hat{g}_{OLS} \over {1+\gamma}} 
$$
也就是说，***岭回归将OLS估计值中的每个回归系数 $\hat{g}_{OLS}$ 向0等比例收缩。***

#### Lasso
Lasso（Tibshirani 1996）应用$L^1$范数进行正则，其目标函数为：
$$
\mathop{min}\limits_{g} \Big[ {1\over N}(y-Xg)'(y-Xg) +\gamma \sum\limits_{j=1}^K |g_j| \Big]
$$

与岭回归不同，**该解并不会线性依赖于 $y$，且该解不存在解析解**，但是部分算法可以求出Lasso的数值解。

类似地，这种正则也会使得回归系数向0收缩，但情况与岭回归稍有不同。***Lasso可以产生稀疏的（Sparse）系数估计值，即向量 $\hat{g}$ 只包括少数个非零元素***。

在 $ X $是 正交矩阵的特殊情况下，Lasso将OLS估计值向零移动一个固定量 $ \gamma $ 【因此，小系数收缩大，大系数收缩小】 。然而，**如果这个操作会导致 $\hat{g}$ 的某个元素的符号发生变化**，那么它将被设置为零。因此有：
$$
\hat{g}_j = sgn(\hat{g}_{j,OLS})(|\hat{g}_{j,OLS}|-\gamma)_+
$$

#### Comparison
当变量之间相关时，Lasso将会遇到问题。

例如，假设我们有两个高度正相关的协变量，他们各自与 $g$ 中的元素相关联。在这种情况下，在非零系数的变量中无论是只包括其中一个协变量（**其系数估计值是真实系数的两倍**）或同时包括这两个协变量（**二者系数估计值都大致等于真实系数**），**对Lasso目标函数中的损失项几乎没有影响**。因此，对于Lasso求解来说，非零系数是只包含其中一个协变量还是两个无关紧要。Lasso具体会得到哪种结果可能取决于一些无关因素，比如数据中的噪声。对数据稍作调整就可能会导致Lasso从选择一个变量转变到另一个变量，**最好的办法是在模型中同时使用两个变量的均值，以便让二者的噪声相互抵消。这正是岭回归的处理方式**。

还有一个重要的问题是：协变量的缩放会改变岭回归、Lasso的估计值。

例如在岭回归中，假设 $X$ 满足 $X'X$ 是对角阵。当我们将 $\gamma I_K$ 添加到 $X'X$ 时，估计值向零收缩幅度的程度取决于 $X'X$ 对角线元素的大小。***如果协变量的方差较小，即 $X'X$ 中的对角线元素较小，将 $\gamma$添加到对角线元素的作用要比协变量方差较大时强得多*【比例】**。

出于这个原因，在进行岭回归估计之前，首先将协变量标准化，使其标准差等于 1 是十分常见的操作。然而，这种处理并非总是正确的。有时，关于学习问题的先验知识会告诉我们，***某些协变量的回归系数比起其他的协变量来说更应该被向零收缩***。


#### Model Selection <!-- {docsify-ignore} -->
对于线性模型可以使用AIC、BIC等信息准则。在岭回归情况下，若$ \epsilon $ 代表不相关的高斯噪声，则AIC定义为：
$$
AIC(\gamma) = Nlogmse(\gamma)+2d(\gamma)
$$
对于非线性模型来说，度量模型复杂度或者有效参数个数比起线性模型来说要困难得多。即使对于线性模型，AIC也依赖于很强的假设。如果上述假设无法满足，也只有在指定了似然函数的确切形式之后才能应用AIC度量。

出于上述原因，机器学习方法一般使用交叉验证，这一方法对于内在假设方面研究较低。但在数据结构性改变的背景下，这一方法的效果仍有待商榷。



### Prior
> [!TIP|label:问题]
> 是否存在一种完全自动化的通用机器学习方法，使其在任何情况下（例如，无论预测问题是关于图像识别、生物医学还是资产定价），都可以仅由数据完全揭示其中的函数关系，从训练数据中归纳出规律并为学习算法没有看到的测试及数据提供预测？

Wolpert(1996)指出这种通用学习算法并不存在。这个结果被称为机器学习中没有免费的午餐(no-free-lunch)定理。也就是说：**除非我们对预测问题有一些*先验知识*，否则没有理由认为一种机器学习算法会优于另一种**。通过先验这一角度，自然引入了贝叶斯的理念。

贝叶斯允许我们通过概率分布的形式表达先验知识。考虑到线性回归框架
$$
y = Xg + \epsilon
$$
并假设 $ \epsilon \sim N(0,\Sigma) $。

***其中我们已知协方差矩阵 $\Sigma$ ，但不知道稀疏向量 $g$ 。先验知识以 $g$ 的先验分布的形式出现。在特定的假设下，我们可以将贝叶斯估计映射到本节所讨论的Ridge和Lasso中，正如接下来展示的，先验分布的选择会影响对目标函数施加的惩罚类型。***

假设我们认为 $g$ 中的元素服从**多元正态分布** $g \sim N(0,\Sigma_g)$。给定这个先验分布 $p(g)$ 以及回归残差 $\epsilon$ 以及回归残差 $\epsilon$ 所隐含的似然函数 $p(y|g)$，贝叶斯定理告诉我们，当给定观测数据 $g$ 时，$g$ 的**后验条件分布**为：
$$
p(g|y)\propto p(y|g) p(g)
$$
当先验分布和似然函数都服从正态分布时，后验分布也服从正态分布。这种后验分布的均值表达式类似于广义最小二乘估计（GLS），不过在求逆矩阵的运算中多了一个附加项：
$$
\hat{g} = (X'\Sigma^{-1}X+\Sigma^{-1}_g)^{-1}
$$
这个附加项会使得估计值偏离GLS的估计值，并向先验分布的均值进行收缩（**假设先验均值是零向量**）。如果进一步设定回归残差满足独立同方差，即 $\Sigma_g = I_K \sigma_g^2$，我们将得到：



$$
\hat{g} = (X'X+{\sigma^2 \over \sigma^2_g}I_K)^{-1}
$$

***该表达式与当 $\gamma = {\sigma^2 \over \sigma^2_g}$ 时的[岭回归](#Ridge)估计量相同。这意味着岭回归的罚项具有贝叶斯解释，它将 $\gamma$ 与先验分布的确定程度联系起来：***

<mark>【这也意味着，只有当先验分布满足同方差，正则项才能与贝叶斯联系起来】</mark>

- 如果 $\sigma^2_g$ 很大，即我们没有关于 $g$ 中元素可能大小的精确先验观点，那么 $\gamma$ 将会很小，因而估计值向先验均值收缩的程度便会很低。
- 如果 $\sigma^2_g$ 很小，即先验分布紧紧地围绕在均值附近，那么估计值向先验均值收缩的程度就很高。

过拟合意味着估计模型的时候没有给予关于参数的先验信息适当的权重。如果先验是扩散的，即 $\sigma_g^2 \rightarrow \infty$ ，那么确实没有收缩的必要。**只有在有理由（例如基于经济学合理性考虑）认为回归系数的幅度不太可能非常大的前提下，讨论过拟合才有意义**。这时，忽略这样的信息并在不使用收缩的情况下估计回归模型，就意味着我们没有对这些先验信息给予任何权重，从而过度拟合了数据。

### Review

贝叶斯框架使我们能够更准确地解释对于过拟合问题的担忧，我们之前以解决它为动机介绍了正则化和收缩。

岭回归可被视为一个带有正态分布先验的贝叶斯回归。如果回归系数的先验协方差矩阵以及随机扰动项的矩阵均正比于单位矩阵，岭回归便等价于这个特殊的贝叶斯回归。

也就是说，只有满足同方差，正则化才能与贝叶斯联系起来。同时，贝叶斯中不同的先验分布也最终决定了实现正则化里不同的收缩效果。

> [!NOTE|label:注意]
> 当 $g$ 的先验分布满足 $\Sigma_g = I_K \sigma_g^2$，即 $g$ 的所有元素满足同方差时，我们可以利用 $\gamma = {\sigma^2 \over \sigma^2_g}$ 将二者联系起来。因此，**岭回归的贝叶斯解释隐含了将回归系数朝着符合同方差的先验收缩**。

正则化的贝叶斯解释也阐明了我们应该如何考虑岭回归中协变量的尺度缩放问题。如果人们想要使用岭回归，那么变量应该以这样一种方式进行缩放，对于缩放后的变量来说，在先验分布中指定 $g$ 的所有元素满足同方差是合理的。

相反，如果我们认为某些系数收缩的幅度可能比其他系数小，那么岭回归将不会产生适当程度的收缩。**相对于那些离零很近的协变量系数，那些距离零较远的协变量系数被岭回归收缩的太多【幅度】**。因此，我们应对协变量重新进行尺度缩放，使得 $g$ 的元素满足同方差这个假设变得合理。这就是我们需要引入特定先验知识的地方，并通过它们来指导估计量。

***在贝叶斯框架中，我们对于 $g$ 的先验分布的观点决定了我们最终是采用岭回归还是其他类型的收缩。***


## Shrinkage
这一章的内容都在为下一章做铺垫
### 预期收益率与协方差

![](../Courses_in_SUSTech/image/20230222ZH1.jpg)


<mark> $ R^2 $ 与夏普比率的背离 </mark>

#### Two performance signal for OLS
股票服从以下数据生成过程：
$$\begin{aligned}
r_t &= \mu + \epsilon_t \\    
\mu &= Xg
\end{aligned}$$
$r_t$ 是第 $t$ 期 $N \times 1$ 维收益率向量， $X$ 是 $N \times K$ 维预测变量矩阵，$\epsilon_t$ 是一个包含 $N$ 个满足独立同分布的随机扰动项的向量，其协方差矩阵为对角阵 $\Sigma = I_N \sigma^2$。

> [!NOTE]
> 在这里可不失一般性地假设预测变量X不随时间变化。为了理解这个假设，最简单的例子是考虑一个只有一个预测变量并且它的取值被转化为相对排序的情况。这时，我们仅需在每个时刻 $t$ 将该预测变量向量和对应的收益率向量 $r_t$ 中的元素重新排序，使得预测变量向量不随时间变化。但是需要强调的是，如果除预测变量的排序外，预测变量本身的取值也能够改变预期收益率的估计，即哪怕对预测变量进行了重新排序后，$\mu_{t-1} = X_{t-1}g $依然是时变的，那么 $X$ 不随时间变化这一假设便不再合适。


通过OLS将 $t=1$ 到 $t= \tau $期的平均收益率 $\overline{r} = {1\over \tau} \sum_{t+1}^{\tau} r_t $对含有 $K$ 个协变量的 $X$ 做回归，来估计一个收益率预测模型。估计系数为：
$$\hat{\mu} = X(X'X)^{-1}X'\overline{r}$$
由 $\overline{r} = \mu + \overline{\epsilon},\ \overline{\epsilon} =  {1\over \tau} \sum_{t+1}^{\tau}\epsilon_t$，则有

$$\begin{aligned}
\hat{\mu} &=  X(X'X)^{-1}X'(Xg+\overline{\epsilon}) \\
&= X(X'X)^{-1}X'Xg+X(X'X)^{-1}X'\overline{\epsilon}\\
&= \mu + u
\end{aligned}$$

其中，$u = X(X'X)^{-1}X'\overline{\epsilon}$，有 $E[u] = 0 $，因此：
$$\begin{aligned}
E[uu'] &= X(X'X)^{-1}X' E[\overline{\epsilon} \ \overline{\epsilon}']X(X'X)^{-1}X' \\
 &= {1\over \tau}X(X'X)^{-1}X' \sigma^2 I_N X(X'X)^{-1}X' \\
&={\sigma^2 \over \tau}X(X'X)^{-1}X'
\end{aligned}$$

接下来，利用 $t = \tau + 1$ 到 $t=T$ 期的数据计算样本外 $R^2$，在测试集上，平均总收益和平均扰动分别为

$$
\overline{r}_v = {1\over T-\tau} \sum_{t=\tau+1}^T r_t, \quad \overline{\epsilon}_v = {1\over T-\tau} \sum_{t=\tau+1}^T \epsilon_t
$$

进而有 $\overline{r}_v = \mu + \overline{\epsilon}_v$,且预测误差为：
$$
\overline{r}_v - \hat{\mu} = \mu + \overline{\epsilon}_v - \hat{\mu} = \overline{\epsilon}_v - u
$$

因此，<mark>样本外 $R^2$ </mark>为
$$\begin{aligned}
R_{OOS}^2 &= 1-{{(\overline{\epsilon}_v - u)(\overline{\epsilon}_v - u)'}\over {(\overline{\epsilon}_v+\mu)(\overline{\epsilon}_v+\mu)'}} \\
& \approx 1-{{1 \over T-\tau}\sigma^2\over {1\over N}\mu'\mu+{1 \over T-\tau}\sigma^2}-{{1 \over \tau}\sigma^2\over {1\over N}\mu'\mu+{1 \over T-\tau}\sigma^2} \tag{3-7}
\end{aligned}$$

相对于 $R^2$ ，投资者和金融经济学家更关心通过充分利用预期收益率截面差异而构造的投资组合的收益率。考虑如下构造权重：
$$
\hat{\omega} = {1\over \sqrt{\hat{\mu}'\hat{\mu}}}\hat{\mu}
$$
尽管该权重能够确保所有股票权重的平方和为1，因而权重的大小是可解释的，但在这种方式之下**多空头寸不再严格相等**。

鉴于我们假设收益率在界面上是不相关的，均值方差有效投资组合（即实现最大夏普比率的组合）中股票的权重与他们的预期收益率成正比。因此， 在这个特殊的情况下，投资权重 $\hat{\omega}$ 同时代表了均值方差有效投资组合中股票权重的估计。

> [!TIP|label:the special portfolio]
> 在均值方差有效投资组合中，资产的权重正比于 $\Sigma^{-1}\mu $。如果我们对权重进行缩放使得他们的平方和等于1，那么权重变为 ${1\over \sqrt{\mu'\Sigma^{-2}\mu}}\Sigma^{-1}\mu $。该投资组合即为最大夏普比率的投资组合。

根据权重 $\hat{\omega}$ 构建的投资组合样本外（即 $t>\tau$）但其收益率的期望和方差分别为：
$$
E[\hat{\omega} \hat{r}_v \ | \ \hat{\omega}] \approx {\mu'\mu \over \sqrt{\mu'\mu + {N\over \tau}\sigma^2}}, \quad var(\hat{\omega} \hat{r}_v \ | \ \hat{\omega}) = {1\over T-\tau}\sigma^2 \tag{3-9}
$$
因此，夏普比率的平方为：
$$
{(E[\hat{\omega} \hat{r}_v \ | \ \hat{\omega}])^2 \over var(\hat{\omega} \hat{r}_v \ | \ \hat{\omega})} \approx ({T-\tau \over \sigma^2}){\mu'\mu \over {(\mu'\mu)^2 + {N\over \tau}\sigma^2}}
$$
在这里，由于我们假设了扰动项的协方差矩阵为对角阵，因此夏普比率的平方近似的正比于式（3-9）所示的投资组合预期收益率平方。所以接下来我们重点关注预期收益率。




正则化下的情况

阐述为何正则化不能有效提高投资组合表现

新方法（new shrinkage），是否还要满足同方差



### 通过构建投资组合估计协方差矩阵

相比于其他典型的机器学习应用，预测误差之间的协方差在资产定价中的影响更大。如果我们的目标是基于收益率预测模型构建高夏普比率的投资组合，那么 ***收益率中不可预测部分的协方差矩阵则显得尤其重要***。

到现在为止，我们一直假设该协方差矩阵是已知的，但在现实中需要对其进行估计，这会**引入额外的预测误差**，进而会对投资组合的构造带来很大的麻烦。极端情况下，如果我们想尝试基于上千只股票构建均值方差有效投资组合，则**需要估计的协方差矩阵中的参数将多达上百万个**。如若不对该矩阵的参数形式施加任何限制或不施加任何收缩，我们基本不可能完成这项估计任务。此外，**典型的股票收益率数据集是非平衡的面板数据**，这增加了估计协方差矩阵的难度。最后，对于一个横跨数十年的数据集而言，随着时间的推移，**个股特征会逐渐变化**，因此股票收益率之间的协方差性质也并非一成不变。

由于个股收益率之间的协方差矩阵难以估计，因此先利用收益率预测模型的协变量将个股聚合成不同的投资组合也许是更合理的做法。如果用于构建投资组合的公司特征与协方差暴露联系紧密，那么这些**投资组合之间的协方差将会比个股之间的协方差要稳定得多**。此外，当协变量的数量小于资产的数量（即K < N时），通过构建投资组合将有助于**更准确地估计协方差矩阵**。

> [!NOTE]
> 在什么条件下，我们能够享受到因将个股聚合为投资组合而带来的协方差矩阵估计方面的好处，但却不必因此牺牲潜在的投资机会呢，即我们最终获得的最大夏普比率平方不会因此而降低。

当股票收益率的生成过程满足 $ r = Xg + \epsilon $，其中可预测的部分为 $\mu = Xg$。接下来，考虑以协变量 $X$ 为权重的投资组合，其已实现的收益率为 $r_p = Xr'$，预期收益率为 $\mu_p = X'\mu = X'Xg$，以及协方差矩阵 $\Sigma_p = X'\Sigma X$ 。在上述定义下，问题变为：

***如果要使通过个股和通过组合所能够获得的最大夏普比率平方相同，那么协变量 $X$ 和收益率协方差矩阵之间需要满足怎样的关系？***

#### 证明
在数学上，上式转化为寻找合适的条件，使得 $\mu'\Sigma^{-1}\mu = \mu'_p\Sigma^{-1}_p\mu_p$，即：
$$
g'X'\Sigma^{-1}Xg=g'X'X(X'\Sigma X)^{-1}X'Xg
$$

根据Amemiya (1985)中的定理6.1.1，上式成立的充要条件是，存在矩阵 $\Psi,\Phi, U$，使得协方差矩阵 $\Sigma$ 满足如下形式：
$$
\Sigma = X\Psi X'+U\Phi U' +\sigma^2 I_N \tag{1}
$$

其中 $U$ 满足 $U'X=0$。对于寻求均值-方差最优化的投资者来说，只有当 $X$ 满足(1)式时，以 $X$ 中的列向量为权重所构建的投资组合才不会有损他们的投资机会。

直观上说，若协方差矩阵 $\Sigma$ 可以被写成式(1)，***则意味着协变量 $X$ 具备以下特征***：
- $X$ 不仅能捕捉股票预期收益率的截面差异，而且还包含了个股在部分系统性因子上的风险暴露信息，这些因子造成了股票收益率的时序变化，如式（1）第一项。【first and second moments are related】
- 除上述因子外，股票在其他任何造成股票收益率时序变化的系统性因子上的暴露 $U$ 均与 $X$ 正交，如式（1）第二项。【只包含风险】
- 最后，除上述两部分之外的任何风险都必须是异质性的，如式（1）第三项。【noise】

***也即不能存在只与收益有关，而与风险无关的套利机会。***

**当我们使用了大量的协变量后，式（1）所列的条件可被认为是近似成立**。比如，假设 $\Sigma$ 中存在 $L$ 维因子结构，即 $\Sigma = G\Omega G' + \sigma^2 I_N$，其中 $G$ 是 $N\times L$ 维因子载荷矩阵而 $\Omega$ 是可逆矩阵。对于典型的股票收益率数据集而言，我们可以用少数个银子（比如 $L \leq 20$）捕捉股票协方差矩阵中的大部分信息。如果 $X$ 含有许多有关股票因子载荷信息的公司特征，则 $G$ 应被 $X$ 近似张成<sup> Tip </sup>，即存在矩阵 $B$ 使得 $G \approx XB$。

<mark>【这一公式代表什么？PCA or Arrow Space？】</mark>

> [!TIP]
> 即我们将 $G$ 的列向量投影到 $X$ 的列向量所张成的空间上。以 $g_i$ 代表 $G$ 的第 $i$ 列向量，以 $X_j$ 代表矩阵 $X$ 的第 $j$ 列向量。我们用线性组合 $\Sigma_j X_j B_{j,i}$ 近似 $g_i$ ，其中 $B_{j,i}$ 是矩阵 $B$ 第 $j$ 行、第 $i$ 列元素。

这时，$\Sigma \approx XB\Omega B'X' + \sigma^2 I_N$，即我们近似地得到式（1）的一个特殊形式。换而言之，如果个股收益率的协方差可以归结为有限几个因子，且我们使用了大量与股票在这些因子上的暴露有关的协变量，那么基于以协变量为权重的投资组合（而非个股）来投资并不会明显地削弱投资机会。

> [!TIP]
> 上述结果针对总体矩而言，也即假设个股真实的预期收益率和协方差矩阵对投资者来说是已知的

将个股聚合为投资组合是我们能够在使用贝叶斯回归方法的同时，将协方差矩阵的估计问题纳入考量，并将先验分布与经济学理论更好地结合在一起。


### 非线性
越来越多的实证证据显示变量之间的交互作能确实是存在的。

利用一系列公司特征作为股票收益率和其二阶矩的预测变量，Chen, Pelger, and Zhu (2019)训练了一个深度神经网络模型。他们发现非线性关系主要以特征之间的交互作用存在。类似地，Gu, Kelly, and Xiu (2020)则通过神经网络和回归树模型发现了同样的现象，即非线性的主要形式为交互作用，而非单一变量自身非线性变换的叠加。Bryzgalova, Pelger, and Zhu (2020)通过决策树模型将股票依照一系列公司特征划分为不同的树节点，并研究了树节点对应的投资组合。利用决策树，该文捕捉到了特征之间的高阶交互作用，并发现这些交互作用在解释股票截面收益率差异时十分重要。Moritz and Zimmermann (2016)发现，即使将预测变量限制在股票自身的历史收益率的函数时，交互作用依然存在。


### 结构性变化
资产定价领域和其他机器学习应用领域最大的区别之一大概要数金融市场中的***数据生成过程可能在经历持续的结构性变化***了。

结构性变化背后的原因是多种多样的。首先，**整体经济本身就在不断经历结构性变化**。在过去数十年间，生产技术、监管政策、以及制度环境均发生了巨大的变化。很难想象在如此背景下，公司特征和未来收益率直接之间能够保持稳定的关系。其次，**投资者会从数据中学习**。任何在过去发现的收益率可预测现象都可能改变投资者的投资行为，进而使得此前存在的预测关系不再成立。历史数据中存在过的可预测性在未来可能无法以同样的形式出现。

### Normal Shrinkage

换句话说，当我们使用标准岭回归时，相当于认可了这个关于 $g$ 的先验分布，即 $g$ 中所有元素先验分布相同。因此，在岭回归中，对协变量的尺度缩放应该以上述关于 $g$ 的先验的隐含假设成立为前提来进行。

换句话说，基于对预测问题和数据的基本了解，我们应该通过缩放预测变量使得他们的系数大小基本一致。根据我们的先验，如果一些变量对预测的贡献度不如其他变量，则应该其降低其截面方差【从而提高 $g$】，以使得其回归系数的大小与其他重要的协变量的回归系数大小处于同一水平【使 $g$ 满足同方差】。


当 $X'X$ 近似地正比于单位矩阵时，岭回归对所有参数的收缩程度大体一样。虽然这么做能改善 $R^2$ 但是却并不会改善投资组合的预期收益率以及夏普比率。为了使正则化能够对改进投资组合表现发挥切实作用，对于参数估计的收缩应该以一种更微妙的方式进行，而非仅仅对所有参数采用同一个收缩比例。在我们考虑的情况下，即 $X'X = I_K$， 收缩虽然降低了估计误差在资产权重中的比例，但是也按同一程度降低了真实预期收益率信号在资产权重中的比例。

> [!NOTE|label:注意]
> Marcus lo pedez de prado

### Shrinking the cross section

**Prior：Kozak, Nagel and Santosh(2018)指出低方差的投资组合很难具备较高的夏普比率。**





