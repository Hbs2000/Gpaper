# Regularization, Bayes and Shrinkage
本节主要内容来自于机器学习与资产定价（2020，Nagel著，王熙、石川译）。

## 从Regularization谈起
### Factor Zoo and Regularization
一直以来，资产定价方面的研究都是通过<mark>关注低维模型来回避资产价格预测变量的高维度问题</mark>。例如，在研究股票截面收益率预测时，学者们仅仅在回归中使用少量公司特征作为解释变量，他们希望使用仅包含少数几个因子的多因子模型来捕捉股票收益率截面上的投资机会。

鉴于为数众多的变量都可能预测股票收益率以及被拿来构造基于公司特征的因子投资组合，在研究中仅仅关注很少数量而因子表明学者们在模型中强加了很强的**稀疏性（Sparsity）**，这意味着将其余成百上千的因子对收益率的影响视为0。

而这些少数得到关注的因子均是被预先假定好的(pre specified)，也即，这一设定很大程度上取决于**研究者的主观研究兴趣以及对于数据的贴合程度**，因此，也被称之为特设稀疏性假设（ad hoc Sparsity）。

这背后也隐含着一个统计问题。当预测股票收益率的变量个数（J）接近或超过能够观测到的股票数量（N）时，常规的统计学方法（例如普通最小二乘法（OLS））并不适用。当数据量并不显著大于预测变量时，OLS回归会过度拟合数据中的噪声，导致样本内和样本外表现差异巨大。

然而，通过贝叶斯方法，过拟合问题得以规避。因为贝叶斯能够自动选取最合适且有效的参数数量。

## Bayes
### 两种正则化方法

在线性回归模型中:
$$
y_i = x_i'g+\epsilon_i
$$
其中$g$代表未知回归系数向量。假设有N个观测值,将其堆叠成$N\times 1$为向量$y = (y_1,y_2,...y_N)'$ 和$N\times K$维矩阵$X=(x_1,x_2....,x_N')$。估计 $g$ 的常用方法为使得误差平方最小,即目标函数为:
$$
\mathop{min}\limits_{g} (y-Xg)'(y-Xg)
$$

将目标函数对$g$ 进行微分，令一阶导数为0并求解$g$ ，便得到OLS估计量：
$$
\hat{g} = (X'X)^{-1}X'y
$$
以及样本内拟合值：
$$
\hat{y} = X\hat{g}
$$

当变量个数K过大时，可以通过正则项 $\hat{g}$来避免其中某些元素的取值幅度过高从而能够改善预测性能。接下里介绍两种本节中经常提及的正则化方法。

> [!NOTE|label:注意]
> 正则化本身具有贝叶斯解释，而这种解释可以被理解为对模型参数施加了某些先验分布。Kozak,Nagel and Santosh(2020)正是在这种贝叶斯解释基础上，将经济学推理和先验知识注入针对资产定价的机器学习方法设计中。

#### Ridge
在岭回归（Hoerl and Kennard 1970）中，优化的目标在最小化与OLS相同的误差平方和损失函数基础上，补充了$L^2$ 正则项 $g'g$：
$$
\mathop{min}\limits_{g} \Big[ {1\over N}(y-Xg)'(y-Xg) +\gamma g'g]
$$
因此，目标函数包括两个部分。第一项代表损失（loss），也就是基础的OLS项。第二项为正则项，其中超参数 $\gamma$ 控制正则的强度，其解为：
$$
\hat{g}=(X'X+\gamma I_k)^{-1} X'y
$$
其中 $I_k$ 是 $K \times K$ 维单位矩阵。通过向 $X'X$ 添加对角矩阵，即<mark>岭</mark>。求逆运算时，$\gamma I_k$ 的存在将导致回归系数向0收缩。直观的说，正则项 $g'g$ 的存在会使得 $\hat{g}$ 中取值幅度过高的元素进行惩罚，因此相比于OLS而言系数更接近0。

**当 $X$ 为正交矩阵**，即 $X'X = I_K$，则有：
$$
\hat{g} = {\hat{g}_{OLS} \over {1+\gamma}} 
$$
也就是说，***岭回归将OLS估计值中的每个回归系数 $\hat{g}_{OLS}$ 向0等比例收缩。***

#### Lasso
Lasso（Tibshirani 1996）应用$L^1$范数进行正则，其目标函数为：
$$
\mathop{min}\limits_{g} \Big[ {1\over N}(y-Xg)'(y-Xg) +\gamma \sum\limits_{j=1}^K |g_j| \Big]
$$

与岭回归不同，**该解并不会线性依赖于 $y$，且该解不存在解析解**，但是部分算法可以求出Lasso的数值解。

类似地，这种正则也会使得回归系数向0收缩，但情况与岭回归稍有不同。***Lasso可以产生稀疏的（Sparse）系数估计值，即向量 $\hat{g}$ 只包括少数个非零元素***。

在 $ X $是 正交矩阵的特殊情况下，Lasso将OLS估计值向零移动一个固定量 $ \gamma $ 【因此，小系数收缩大，大系数收缩小】 。然而，**如果这个操作会导致 $\hat{g}$ 的某个元素的符号发生变化**，那么它将被设置为零。因此有：
$$
\hat{g}_j = sgn(\hat{g}_{j,OLS})(|\hat{g}_{j,OLS}|-\gamma)_+
$$

#### Comparison
当变量之间相关时，Lasso将会遇到问题。

例如，假设我们有两个高度正相关的协变量，他们各自与 $g$ 中的元素相关联。在这种情况下，在非零系数的变量中无论是只包括其中一个协变量（**其系数估计值是真实系数的两倍**）或同时包括这两个协变量（**二者系数估计值都大致等于真实系数**），**对Lasso目标函数中的损失项几乎没有影响**。因此，对于Lasso求解来说，非零系数是只包含其中一个协变量还是两个无关紧要。Lasso具体会得到哪种结果可能取决于一些无关因素，比如数据中的噪声。对数据稍作调整就可能会导致Lasso从选择一个变量转变到另一个变量，**最好的办法是在模型中同时使用两个变量的均值，以便让二者的噪声相互抵消。这正是岭回归的处理方式**。

还有一个重要的问题是：协变量的缩放会改变岭回归、Lasso的估计值。

例如在岭回归中，假设 $X$ 满足 $X'X$ 是对角阵。当我们将 $\gamma I_K$ 添加到 $X'X$ 时，估计值向零收缩幅度的程度取决于 $X'X$ 对角线元素的大小。***如果协变量的方差较小，即 $X'X$ 中的对角线元素较小，将 $\gamma$添加到对角线元素的作用要比协变量方差较大时强得多*【比例】**。

出于这个原因，在进行岭回归估计之前，首先将协变量标准化，使其标准差等于 1 是十分常见的操作。然而，这种处理并非总是正确的。有时，关于学习问题的先验知识会告诉我们，***某些协变量的回归系数比起其他的协变量来说更应该被向零收缩***。


#### Model Selection <!-- {docsify-ignore} -->
对于线性模型可以使用AIC、BIC等信息准则。在岭回归情况下，若$ \epsilon $ 代表不相关的高斯噪声，则AIC定义为：
$$
AIC(\gamma) = Nlogmse(\gamma)+2d(\gamma)
$$
对于非线性模型来说，度量模型复杂度或者有效参数个数比起线性模型来说要困难得多。即使对于线性模型，AIC也依赖于很强的假设。如果上述假设无法满足，也只有在指定了似然函数的确切形式之后才能应用AIC度量。

出于上述原因，机器学习方法一般使用交叉验证，这一方法对于内在假设方面研究较低。但在数据结构性改变的背景下，这一方法的效果仍有待商榷。



### Prior
> [!TIP|label:问题]
> 是否存在一种完全自动化的通用机器学习方法，使其在任何情况下（例如，无论预测问题是关于图像识别、生物医学还是资产定价），都可以仅由数据完全揭示其中的函数关系，从训练数据中归纳出规律并为学习算法没有看到的测试及数据提供预测？

Wolpert(1996)指出这种通用学习算法并不存在。这个结果被称为机器学习中没有免费的午餐(no-free-lunch)定理。也就是说：**除非我们对预测问题有一些*先验知识*，否则没有理由认为一种机器学习算法会优于另一种**。

通过先验这一角度，自然引入了贝叶斯的理念。

贝叶斯允许我们通过概率分布的形式表达先验知识。考虑到线性回归框架
$$
y = Xg + \epsilon
$$
并假设 $ \epsilon \sim N(0,\Sigma) $。

***其中我们已知协方差矩阵 $\Sigma$ ，但不知道稀疏向量 $g$ 。先验知识以 $g$ 的先验分布的形式出现。在特定的假设下，我们可以将贝叶斯估计映射到本节所讨论的Ridge和Lasso中，正如接下来展示的，先验分布的选择会影响对目标函数施加的惩罚类型。***

假设我们认为 $g$ 中的元素服从**多元正态分布** $g \sim N(0,\Sigma_g)$。给定这个先验分布 $p(g)$ 以及回归残差 $\epsilon$ 以及回归残差 $\epsilon$ 所隐含的似然函数 $p(y|g)$，贝叶斯定理告诉我们，当给定观测数据 $g$ 时，$g$ 的**后验条件分布**为：
$$
p(g|y)\propto p(y|g) p(g)
$$
当先验分布和似然函数都服从正态分布时，后验分布也服从正态分布。这种后验分布的均值表达式类似于广义最小二乘估计（GLS），不过在求逆矩阵的运算中多了一个附加项：
$$
\hat{g} = (X'\Sigma^{-1}X+\Sigma^{-1}_g)^{-1}
$$
这个附加项会使得估计值偏离GLS的估计值，并向先验分布的均值进行收缩（**这里我们假设先验均值是零向量**）。如果进一步设定回归残差满足独立同方差，即 $\Sigma_g = I_K \sigma_g^2$，我们将得到：



$$
\hat{g} = (X'X+{\sigma^2 \over \sigma^2_g}I_K)^{-1}
$$

***该表达式与当 $\gamma = {\sigma^2 \over \sigma^2_g}$ 时的[岭回归](#Ridge)估计量相同。这意味着岭回归的罚项具有贝叶斯解释，它将 $\gamma$ 与先验分布的确定程度联系起来：***

<mark>【这也意味着，只有当先验分布满足同方差，正则项才能与贝叶斯联系起来】</mark>

- 如果 $\sigma^2_g$ 很大，即我们没有关于 $g$ 中元素可能大小的精确先验观点，那么 $\gamma$ 将会很小，因而估计值向先验均值收缩的程度便会很低。
- 如果 $\sigma^2_g$ 很小，即先验分布紧紧地围绕在均值附近，那么估计值向先验均值收缩的程度就很高。

过拟合意味着估计模型的时候没有给予关于参数的先验信息适当的权重。如果先验是扩散的，即 $\sigma_g^2 \rightarrow \infty$ ，那么确实没有收缩的必要。**只有在有理由（例如基于经济学合理性考虑）认为回归系数的幅度不太可能非常大的前提下，讨论过拟合才有意义**。这时，忽略这样的信息并在不使用收缩的情况下估计回归模型，就意味着我们没有对这些先验信息给予任何权重，从而过度拟合了数据。

### Review

贝叶斯框架使我们能够更准确地解释对于过拟合问题的担忧，我们之前以解决它为动机介绍了正则化和收缩。

实际上正则化本身具有贝叶斯解释，在不同先验的情况下，

正则化的贝叶斯解释也阐明了我们应该如何考虑岭回归中协变量的尺度缩放问题。如果人们想要使用岭回归，那么变量应该以这样一种方式进行缩放，对于缩放后的变量来说，在先验分布中指定 $g$ 的所有元素满足同方差是合理的。

岭回归可被视为一个带有正态分布先验的贝叶斯回归。如果回归系数的先验协方差矩阵以及随机扰动项的矩阵均正比于单位矩阵，岭回归便等价于这个特殊的贝叶斯回归。换句话说，当我们使用标准岭回归时，相当于认可了这个关于 $g$ 的先验分布，即 $g$ 中所有元素先验分布相同。因此，在岭回归中，对协变量的尺度缩放应该以上述关于 $g$ 的先验的隐含假设成立为前提来进行。

换句话说，基于对预测问题和数据的基本了解，我们应该通过缩放预测变量使得他们的系数大小基本一致。根据我们的先验，如果一些变量对预测的贡献度不如其他变量，则应该其降低其截面方差【从而提高 $g$】，以使得其回归系数的大小与其他重要的协变量的回归系数大小处于同一水平【同方差】。

缩放是为了满足同方差。


> [!NOTE|label:注意]
> 当 $g$ 的先验分布满足 $\Sigma_g = I_K \sigma_g^2$，即 $g$ 的所有元素满足同方差时，我们可以利用 $\gamma = {\sigma^2 \over \sigma^2_g}$ 将二者联系起来。因此，**岭回归的贝叶斯解释隐含了将回归系数朝着符合同方差的先验收缩**。



相反，如果我们认为某些系数收缩的幅度可能比其他系数小，那么岭回归将不会产生适当程度的收缩。**相对于那些离零很近的协变量系数，那些距离零较远的协变量系数被岭回归收缩的太多【绝对值】**。因此，我们应对协变量重新进行尺度缩放，使得 $g$ 的元素满足同方差这个假设变得合理。这就是我们需要引入特定先验知识的地方，并通过它们来指导估计量。


也就是说，只有满足同方差，正则化才能与贝叶斯联系起来。同时，贝叶斯中不同的先验分布也最终决定了实现正则化里不同的收缩效果。

**在贝叶斯框架中，我们对于 $g$ 的先验分布的观点决定了我们最终是采用岭回归还是其他类型的收缩。**

例如，如果先验是拉普拉斯分布而不是正态分布，那么我们会得到类似Lasso的效果。

【但是无论是Lasso还是Ridge，都是通过系数大小来进行Shrinkage，系数大小是否隐含着方差大小？】


同方差与正交





## Shrinkage

### Normal Shrinkage


当 $X'X$ 近似地正比于单位矩阵时，岭回归对所有参数的收缩程度大体一样。虽然这么做能改善 $R^2$ 但是却并不会改善投资组合的预期收益率以及夏普比率。为了使正则化能够对改进投资组合表现发挥切实作用，对于参数估计的收缩应该以一种更微妙的方式进行，而非仅仅对所有参数采用同一个收缩比例。在我们考虑的情况下，即 $X'X = I_K$， 收缩虽然降低了估计误差在资产权重中的比例，但是也按同一程度降低了真实预期收益率信号在资产权重中的比例。

> [!NOTE|label:注意]
> Marcus lo pedez de prado

### Shrinking the cross section

**Prior：Kozak, Nagel and Santosh(2018)指出低方差的投资组合很难具备较高的夏普比率。**





