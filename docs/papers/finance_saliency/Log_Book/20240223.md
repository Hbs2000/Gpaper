# Frequency on different dimensions

在多因子的频率计算中，还要考虑个股和不同因子分解后，频率重叠的问题。这是因为不同因子之间并不是正交的，因此，可以参考 shrinking 的做法，将因子做正交化处理，这样计算不同频率的协方差分解时，各频率在不同维度上就是独立的。

## Shrinking

shrinking 的构造过程如下：

首先计算各因子之间的协方差，接着对协方差矩阵进行特征值分解：

$$
\begin{equation}
    \Sigma = Q D Q' \quad \text{with} \ D =(d_1,d_2,\cdots,d_H)
\end{equation}
$$

其中 $Q$ 是 $\Sigma$ 的特征向量矩阵，$D$ 是 $\Sigma$ 的对角特征值矩阵，沿对角线从大到小排列。

接着，构造主成分因子 （PC factors），

$$
\begin{equation}
    p_t = Q'f_t,
\end{equation}
$$

因此，随机折现因子可表示为

$$
\begin{equation}
    M_t = 1 - b_p' (p_t - E[p_t]), \quad \text{where } b_p = D^{-1} E[p_t]
\end{equation}
$$

此处非常重要的一点在于，将因子 $f_t$ 转化为主成分因子 $p_t$ 后，风险价格仍然满足 SDF 约束，即 $b_p = D^{-1} E[p_t]$。

我们进一步来看具体的推导过程：

$$
\begin{aligned}
M_t &=  1- b' (f_t - E[f_t]) \\
&= 1- b'Q (Q'f_t - Q'E[f_t]) \\
&= 1- b'Q (p_t - E[p_t]) \\
&= 1 - b_p'(p_t - E[p_t])
\end{aligned}
$$

最后一行 $b_p' = b'Q$ 的推导如下 

$$
\begin{aligned}
b &= \Sigma ^{-1} E[f_t] \\
\Rightarrow b'Q &= (E[f_t])' (\Sigma ^{-1})' Q
\end{aligned}
$$

因为 $\Sigma = Q D Q', Q' = Q^{-1}$，因此 $\Sigma^{-1} = Q' D^{-1} Q$，带入则有

$$
\begin{aligned}
b'Q &= (E[f_t])' Q D^{-1} Q' Q = (E[f_t])' Q D^{-1}  \\
&= (Q'E[f_t])'  D^{-1} = (E[p_t])' D^{-1} = b_p
\end{aligned} 
$$

然而，此处推导的起始，也是整个推导的核心部分在于式（1），**由于 shrinking 中 test assets 和 factor 相同**，因此式（1）中的 $\Sigma$ 是方阵，故而使用特征值分解。

## SVD

但是，当 test assets 和 factor 不同时，$\Sigma$ 就不是方阵，维度变为 $N \times K$，$N$ 是 test assets 的维度，$K$ 是 factor 的维度，此时推导过程就会发生变化。

首先，对非方阵，无法进行特征值分解，而只能进行奇异值分解：

$$
\begin{equation}
    \underbrace{\Sigma}_{N \times K} = \underbrace{U}_{N \times N} \underbrace{D}_{N \times K} \underbrace{V^T}_{K \times K}
\end{equation}
$$

同样地，将 $V^T$ 作为因子的权重，构建 PC factors

$$
\begin{equation}
    p_t = V^T f_t
\end{equation}
$$

沿用相同的逻辑，计算随机折现因子：

$$
\begin{aligned}
M_t &=  1- b^T (f_t - E[f_t]) \\
&= 1- b^TV (V^Tf_t - V^TE[f_t]) \\
&= 1- b^TV (p_t - E[p_t]) 
\end{aligned}
$$

涉及到 $b'V$ 的计算时，情况有些不同了，

$$
b^TV = (E[R_t])^T(\Sigma^{-1})^T V 
$$

因为 $\Sigma^{-1} = V D^{-1} U^T$, $(\Sigma^{-1})^T = U (D^{-1})^T V^T$，带入得

$$
b^TV = (E[R_t])^T U D^{-1} V^T V = (U^T E[R_t])^T D^{-1} 
$$

所以此时相当于，不仅通过 $V^T f_t$ 构造了 PC factors，还通过 $U^T E[R_t]$ 构造了 PC assets，把 test assets 和 factors 同时旋转到了不同的空间。而 shrinking 中因为 test assets 和 factors 相同，所以可以同时使用 $Q$ 将二者旋转到同一空间。

此时的协方差，指的就是定价因子和 test assets 之间的协方差。

## Moore-Penrose inverse

然而此处存在一个基本的问题，**只有方阵才有逆**。这是一个比较普遍的问题，所以很早也就有了解，对于非方阵来说，可以计算**广义逆**，也称作**伪逆**。

对于方程组 $Ax = b$ 来说，当自变量的个数与方程的个数一致，此时该方程组有唯一解；当自变量的个数多于方程的个数，称之为**欠定方程组**，“欠”代表约束给少了，故此时有多解或无穷解，而当方程组的个数多于自变量的个数，则称为**超定方程组**，“超”代表约束给多了，可能无解。

对于我们所研究的公式 

$$
\mu = \Sigma b
$$

来说，$\mu$ 代表了方程的个数 $N$，$b$ 代表了因子的个数 $k$，$N >> K$，因此属于超定方程组。当资产个数多于因子个数，则协方差矩阵不为方阵，当协方差矩阵不为方阵，则需要应用伪逆。所以在 SDF 的设定下，**伪逆是无法避免的**。

> A common use of the pseudoinverse is to compute a "**best fit**" (least squares) solution to a system of linear equations that lacks a solution. 
> 
> Another use is to find the **minimum (Euclidean) norm solution** to a system of linear equations with multiple solutions. 

The Moore-Penrose pseudo-inverse 是比较常用的伪逆，可以应用于任何矩阵，无论其维度或奇异与否。

因此，在上文的计算中，$\Sigma = U D V^T$，计算其伪逆

$$
\Sigma^+ = V D^+ U^T
$$

则 $(\Sigma^+)^T = U (D^+)^T V^T$，因此带入有

$$
b^T V = (E[R_t])^T U (D^+)^T V^T V = (U^T E[R_t])^T (D^+)^T = b_p^T
$$

## Methodology

因此，估计流程为

1. 选取过去五年的个股收益率数据和因子收益率数据
2. 计算二者协方差 $\Sigma$，维度为 $N \times K$
3. 进行奇异值分解 $\Sigma = U D V^T$
4. 通过 $(U^T E[R_t])^T D^+$ 得到风险溢价
5. 根据得到的权重，计算**个股收益率**与 PC factors 协方差并进行频率分解 【错误，应该是旋转后的定价因子和 factors 之间的协方差】

方案有一些问题，$D^+$ 的维度如果为 $N \times K$，并且 $N$ 远大于 $K$，那么就属于列满秩，只有 最上面的 $K\times K$ 是对角阵，其他全部是零，所以提出另一种形式。


## Another form

1. 对 $N \times K$ 维的协方差矩阵进行奇异值分解：

$$
\begin{equation}
    \underbrace{\Sigma}_{N \times K} = \underbrace{U}_{N \times N} \underbrace{D}_{N \times K} \underbrace{V^T}_{K \times K}
\end{equation}
$$


2. 构建 PC factors

$$
\begin{equation}
    p_t = V^T f_t
\end{equation}
$$


3. 得到 SDF 的新表达形式

$$
\begin{aligned}
M_t &=  1- b^T (f_t - E[f_t]) \\
&= 1- b^TV (V^Tf_t - V^TE[f_t]) \\
&= 1- b^TV (p_t - E[p_t]) 
\end{aligned}
$$

其中 $b^TV$ 的推导如下：

$$
b^TV = (E[R_t])^T(\Sigma^{-1})^T V 
$$

因为 $\Sigma^{-1} = V D^{-1} U^T$, $(\Sigma^{-1})^T = U (D^{-1})^T V^T$，带入得

$$
b^TV = (E[R_t])^T U (D^{-1})^T V^T V = (E[R_t])^T U (D^{-1})^T
$$

因为 $\text{Cov}(R,f) = \Sigma$，所以 **$\text{Cov}(R,V^Tf) = \Sigma V = UD$**，因此

$$
\begin{equation}
\begin{aligned}
        b^TV &= (E[R_t])^T U (D^{-1})^T = (E[R_t])^T (U^{-1})^T (D^{-1})^T \\
        &= (E[R_t])^T (D^{-1} U^{-1})^T = (E[R_t])^T ((UD)^{-1})^T
\end{aligned}
\end{equation}
$$

将其表示为 $b_v$，则有

$$
\begin{equation}
    \begin{aligned}
        b_v^T = (E[R_t])^T ((UD)^{-1})^T \\
        \Longrightarrow E[R_t] = UD b_v = \Sigma_{vf} b_v
    \end{aligned}
\end{equation}
$$

至此，就完成了不旋转 assets，而仅将因子表示为不同维度的 SDF 形式。并且此时得到的协方差矩阵 $\Sigma_{vf}$ 

## SVD and PCA

### 理论

如果 $A$ 是 $n \times n$ 维对称方阵，那么 $A$ 就有实特征值 $\lambda_1,\cdots,v_n$ 以及正交基 $v_1,\cdots,v_n$，其中每一个向量 $v_i$ 都是 $A$ 的一个特征向量，对应特征值为 $\lambda_1$。于是有

$$
A = PDP^{-1}
$$

其中矩阵 $P$ 的列为 $v_1,\cdots,v_n$，并且 $D$ 是对角矩阵，对角元素为 $\lambda_1,\cdots,\lambda_n$，既然特征向量为正交向量，那么矩阵 $P$ 也是正交矩阵 $P^T P = I$，所以上式也可以写成

$$
A = PDP^T
$$

SVD 可以理解为特征值分解的泛化情况，当 $A$ 不为对称阵或方阵时，仍然可以如此分解。

#### Singular Values <!-- {docsify-ignore} -->

对于 $m \times n$ 维矩阵 $A$ 来说，矩阵 $A^TA$ 为 $n \times n$ 维对称矩阵，所以其特征值为实数，记为 $\lambda$，则 $\lambda \geq 0$。

将矩阵 $A^TA$ 的特征值表示为 $\lambda_1,\cdots,\lambda_n$，排序为 $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_n \geq 0$。令 $\sigma_i = \sqrt{\lambda_i}$，所以 $\sigma_1 \geq \sigma_2 \geq \sigma_n \geq 0$，这些 $\sigma_i$ 就称之为**奇异值**，并且，**奇异值的数目就等于矩阵 $A$ 的秩**。

#### Singular value decomposition  <!-- {docsify-ignore} -->

$m \times n$ 维矩阵 $A$ 的秩为 $r$，$A$ 的 SVD 为：

$$
A = U DV^T
$$

其中，

1. $U$ 是 $m \times m$ 维正交矩阵
2. $V$ 是 $n \times n$ 维正交矩阵
3. $\Sigma$ 是 $m \times n$ 维矩阵，其中第 $i$ 个对角元素等于第 $i$ 个奇异值，$\Sigma$ 的其他所有元素都为零。

#### The relationship  <!-- {docsify-ignore} -->

对于 $m \times n$ 维矩阵 $A$，可以得到 $n \times n$ 维的方阵 $A^TA$，既然是方阵，就可以进行特征值分解，得到特征值和特征向量：

$$
(A^TA)v_i = \lambda_i v_i
$$

将 $A^TA$ 的所有特征向量组合成一个 $n \times n$ 维的矩阵 $V$，这也就是 SVD 中的 $V$ 矩阵了，一般将 $V$ 中的每个特征向量叫做 $A$ 的右奇异向量。

同理，对于 $m \times m$ 维方阵 $AA^T$，可以得到：

$$
(AA^T)u_i = \lambda_i u_i
$$

将 $AA^T$ 的所有特征向量组合成一个 $m \times m$ 维的矩阵 $V$，这也就是 SVD 中的 $U$ 矩阵，其中的每一个特征向量称之为左奇异向量。

**证明** (以 $AA^T$ 为例)

$$
\begin{aligned}
    A &= UDV^T,A^T = VD^TU^T \\
    \Longrightarrow A^TA &= VD^TU^T UDV^T = V D^2V^T
\end{aligned}
$$

从这里也可以看出奇异值与特征值之间的关系，特征值是奇异值的平方

$$
\sigma_i = \sqrt{\lambda_i}
$$

#### SVD and PCA <!-- {docsify-ignore} -->

假设我们有一个去中心化的数据矩阵 $X \in \mathbb{R}^{n\times p}$，其中 $n$ 是样本数，$p$ 是特征数。

PCA 旨在找到一个正交变换，将数据投影到一个新的坐标系中，使得在新坐标系中的方差最大化。这可以通过对协方差矩阵 $C = \frac{1}{n-1}X^TX$ 进行特征分解来实现。意为，协方差矩阵的特征向量对应于主成分的方向，而特征值对应于这些方向上的方差。

因为 $X$ 可以进行奇异值分解 $X = U\Sigma V^T$，所以协方差矩阵可以写成

$$
C = \frac{1}{n-1}X^TX = \frac{1}{n-1} V \Sigma^2 V^T
$$

因为 $X^TXV = V\Sigma^2$，这意味着 $V$ 的列向量是协方差矩阵 $C$ 的特征向量，也即， **$V$ 的列向量就是 PCA 的主成分方向** 。

**所以这一方法还有个问题就在于，$V$ 该如何解释，的确是一个正交基，但是代表了什么主成分？**



