# 20241218

**为什么 $\frac{x_t + x_{t-1}}{2}$ 代表低频，$\frac{x_t - x_{t-1}}{2}$ 代表高频？**

我们用 $A_t$ 和 $D_t$ 来表示低频和高频信息，

$$
A_t = \frac{x_t+x_{t-1}}{2}, \quad B_t = \frac{x_t-x_{t-1}}{2}
$$

我们可以直接分析低频序列 $A_t$ 和高频序列 $D_t$ 的频率成分，来回答这个问题，对低频进行傅里叶分解，有

$$
\begin{aligned}A_f &=\sum_{t=-\infty}^\infty A_te^{-i2\pi f t}\\&=\sum_{t=-\infty}^\infty\frac{x_t+x_{t-1}}{2}e^{-i2\pi f t}\\&=\frac{1}{2}\sum_{t=-\infty}^\infty x_te^{-i2\pi f t}+\frac{1}{2}\sum_{t=-\infty}^\infty x_{t-1}e^{-i2\pi f t}\\&=\frac{1}{2}X(e^{i2\pi f})+\frac{1}{2}e^{-i2\pi f}X_f \\&=\frac{1+e^{-i2\pi f}}{2}X_f \end{aligned}
$$

> $X_f =\sum_{t=-\infty}^\infty x_t e^{-i 2\pi ft}$

同理

$$
D_f = \frac{1 - e^{-i2\pi f}}{2} X_f
$$

接着我们可以分析频率成分中幅度的变化来分析其特性

$$
\begin{aligned}H_{A_f}&=\frac{1+e^{-i2 \pi f}}{2}\\&=\frac{e^{i2 \pi f/2}+e^{-i2 \pi f/2}}{2}e^{-i2 \pi f/2}\\&=e^{-i2 \pi f/2}\cdot\frac{e^{i2 \pi f/2}+e^{-i2 \pi f/2}}{2}\\&=e^{-i2 \pi f/2}\cdot\cos\left(\frac{2 \pi f}{2}\right)\end{aligned}
$$

then magnitude response is 

$$
|H_{A_f}|=\left|\cos\left(\frac{2\pi f}{2}\right)\right|
$$

which means

- At $f = 0, |H_{A_f}| = 1$
- At $f = 0.5, |H_{A_f}| = 0 $

**The cosine term $\cos (\frac{\pi}{2})$ smoothly transitions from $1$ to $0$ as $f$ increases from $0$ to $0.5$, which means the $A_t$ captures more low frequency information.**

Following the same procedure

$$
\begin{aligned}H_{D_f}&=\frac{1-e^{-i2 \pi f}}{2}\\&=\frac{e^{i2 \pi f/2}-e^{-i2 \pi f/2}}{2}e^{-i2 \pi f/2}\\&=e^{-i2 \pi f/2}\cdot\frac{e^{i2 \pi f/2}-e^{-i2 \pi f/2}}{2}\\&=e^{-i2 \pi f/2}\cdot i\sin\left(\frac{2 \pi f}{2}\right)\end{aligned}
$$

then 

$$
|H_{D_f}|=\left|\sin\left(\frac{2 \pi f }{2}\right)\right|
$$

which means

- At $f = 0, |H_{D_f}| = 0$
- At $f = 0.5, |H_{D_f}| = 1 $

Similarly, **The sine term $\sin (\frac{2 \pi f}{2})$ smoothly transitions from $0$ to $1$ as $f$ increases from $0$ to $0.5$, which means the $D_t$ captures more high frequency information.**

This behavior characterize a high-pass fitler and low-pass filter respectively.

## Wold representation

Any covariance-stationary time series can be expressed as an infinite linear combination of **white noise** innovations. This decomposition separates the predictable structure of the series from its inherent randomness. Intuitively, it suggests that past shocks (innovations) propogate through the system, influencing future values in a decaying manner.

A stochastic process $X_t$ is covariance-stationary means

- mean $EX_t$ is constant over time
- covariance $Cov(X_t, X_{t+h})$ depends only on the lag $h$, not on $t$

then any covariance-stationary process $X_t$ can be decomposed as

$$
X_t=\sum_{k=0}^\infty\psi_k\varepsilon_{t-k}
$$

- $\varepsilon_t$ is a sequence of uncorrelated random innovations with $E(\varepsilon_t) = 0$ and $Var(\varepsilon_t) = 1$
- $\phi_k$ are coefficients such that $\sum_{k=0}^\infty\psi_k^2<\infty.$

**Limitations**

1. Lack of Structure insights: it doesn't inherently provide a decomposition into meaningful structural components, such as cycles of different frequencies or orthogonal factors.


## Extended Wold decomposition

The extended Wold representation builds upon the classical theorem by partitioning the process into components that oscillate at different frequencies and ensuring these components are orthogonal.

### A scalar process

> **This procedure is applicable to any process for which a classical Wold representation applies**

The wold representation of $\bold{x}$ states that exists a unit variance, zero mean white noise process $\bold\{\epsilon_t\}_{t\in \mathbb{Z}}$ such that, for any $t$ in $\mathbb{Z}$,

$$
x_t=\sum_{k=0}^{+\infty}\alpha_k\varepsilon_{t-k},
$$

Let us now introduce scales and define the innovation process at scale j with $j \in \mathbb{N}$. If $j = 1$, the innovation process at scale $1$, denoted by $\varepsilon^{(1)}=\left\{\varepsilon_{t}^{(1)}\right\}_{t\in\mathbb{Z}}$, is defined as the process whose terms are 

$$
\varepsilon_t^{(1)}=\frac{\varepsilon_t-\varepsilon_{t-1}}{\sqrt{2}}.
$$

We observe that $\varepsilon_t^{(1)}$ has a zero mean and its variance is equal to $1$, for all $t$. More generally, we define the innovation process at scale $j$ as the process $\varepsilon^{(j)}=\left\{\varepsilon_{t}^{(j)}\right\}_{t\in\mathbb{Z}}$, with

$$
\varepsilon_t^{(j)}=\frac{1}{\sqrt{2^j}}\left(\sum_{i=0}^{2^{j-1}-1}\varepsilon_{t-i}-\sum_{i=0}^{2^{j-1}-1}\varepsilon_{t-2^{j-1}-i}\right).
$$

> [!TIP]
$$
\begin{aligned}
j = 1 : \quad &\frac{\varepsilon_t - \varepsilon_{t-1}}{\sqrt{2}} \\
j=2   : \quad & \frac{1}{\sqrt{4}} ( \sum_{i=0}^{1}\varepsilon_{t-i} - \sum_{i=0}^{1}\varepsilon_{t-2-i} )\\
& = \frac{(\varepsilon_t + \varepsilon_{t-1}) - (\varepsilon_{t-2}+\varepsilon_{t-3})}{\sqrt{4}}
\end{aligned}
$$
> 

The structure of the shocks $\boldsymbol{\varepsilon}^{(j)}$ is that of a **Discrete Haar Transform**. Once a scale level $j$ is set, one may consider the sub-series of $\boldsymbol{\varepsilon}^{(j)}$ defined on the support $S_{t}^{(j)}=\{t-k2^{j}:k\in\mathbb{Z}\}$. The process $\boldsymbol{\varepsilon}^{(j)}$ is moving average of order $2^j -1$ with repect to the Wold innovations of $\mathbf{x}$. 

> [!TIP]
> Support set means, we are doing sub-sampling. For example, when $j=2$, the original series would be 
$$
\varepsilon_t^{(2)},\varepsilon_{t-1}^{(2)},\varepsilon_{t-2}^{(2)},\ldots
$$
> however, that will cause overlaping within the series, so we sub-sample to prevent this phenomenon
$$
\varepsilon_t^{(2)},\varepsilon_{t-4}^{(2)},\varepsilon_{t-8}^{(2)},\ldots.
$$
> the interval is depend on the scale $j$.

Technically, we are decomposing the space $\left\{\varepsilon_{t-k2^j}^{(j)}\right\}_{k\in\mathbb{Z}}$ of infinite moving average whose underlying white noise process $\mathbf{\varepsilon}$

$$
\mathcal{H}_{t}(\boldsymbol{\varepsilon})=\left\{\sum_{k=0}^{+\infty}a_{k}\varepsilon_{t-k}:\quad\sum_{k=0}^{+\infty}a_{k}^{2}<+\infty\right\},
$$

> This space consists of all possible linear combinations of the original Wold innovations ${\epsilon_t}$ that have finite energy.
>
> And it serves as the foundational space from which we will construct orthogonal subspaces.

into orthogonal subspaces

$$
\mathcal{W}_{t}^{(j)}=\left\{\sum_{k=0}^{+\infty}b_{k}^{(j)}\varepsilon_{t-k2^{j}}^{(j)}\in\mathcal{H}_{t}(\boldsymbol{\varepsilon}):\quad\sum_{k=0}^{+\infty}b_{k}^{(j)2}<+\infty\right\},
$$

with $j=1,\cdots,+\infty$.

> [!TIP]
> Within the series, there is no overlapping, and across the series, there are orthogonal, which is the property of Discrete Haar Transform. For instance, $j=1, j=2$, zero mean then 
$$
Cov(\varepsilon_t^{(1)},\varepsilon_t^{(2)})=E[\varepsilon_t^{(1)}\varepsilon_t^{(2)}]
$$
> then
$$
(\varepsilon_t-\varepsilon_{t-1})(\varepsilon_t+\varepsilon_{t-1}-\varepsilon_{t-2}-\varepsilon_{t-3})=\varepsilon_t\varepsilon_t+\varepsilon_t\varepsilon_{t-1}-\varepsilon_t\varepsilon_{t-2}-\varepsilon_t\varepsilon_{t-3}-\varepsilon_{t-1}\varepsilon_t \\ -\varepsilon_{t-1}\varepsilon_{t-1}+\varepsilon_{t-1}\varepsilon_{t-2}+\varepsilon_{t-1}\varepsilon_{t-3}.
$$
> so the expectation would be
$$
E[(\varepsilon_t-\varepsilon_{t-1})(\varepsilon_t+\varepsilon_{t-1}-\varepsilon_{t-2}-\varepsilon_{t-3})]=1+0-0-0-0-1+0+0=0.
$$
> then the covariance is zero between different scale.


We now turn to Wold coefficients at scale $j$. The process $\mathbf{x}$ is contained in $\mathcal{H}_t(\boldsymbol{\varepsilon})$. Therefore, the decomposition of the space $\mathcal{H}_t(\boldsymbol{\varepsilon})$ induces the following representation of $\mathbf{x}$:

$$
x_t=\sum_{i=1}^{+\infty}x_t^{(j)},
$$

where each $x_t^{(j)}$ is the projection of $x_t$ on the subspace $\mathcal{W}_{t}^{(j)}$, with $j\in \mathbf{N}$, and the equality is, again, in the $L^2$ norm. By construction, for a fixed $t$, the components $x_t^{(j)}$ are orthogonal to each other. Since each $x_t^{(j)}$ belongs to $\mathcal{W}_{t}^{(j)}$, we have that

$$
x_{t}^{(j)}=\sum_{k=0}^{+\infty}\psi_{k}^{(j)}\varepsilon_{t-k2^{j}}^{(j)},
$$

for some square-summable sequence of real coefficients $\left\{\psi_k^{(j)}\right\}_{k\in\mathbb{N}_0}$.

Each coeffients $\psi_k^{(j)}$ is obtained by projecting $\mathbf{x}$ on the linear subspacegenerated by the (scale-specfic) innovations $\varepsilon_{t-k2^j}^{(j)}$:

$$
\psi_k^{(j)}=\mathbb{E}\left[x_t\varepsilon_{t-k2^j}^{(j)}\right].
$$

> $\varepsilon_{t-k2^j}^{(j)}$ is the basis vector, and coefficients $\psi_k^{(j)}$ represents how much of $x_t$ lies in the direction of the basis vector. 

Substituting the expression $x_t^{j}$ then we arrive at the extended Wold representation of $x_t$, that is

$$
x_t=\sum_{j=1}^{+\infty}\sum_{k=0}^{+\infty}\psi_k^{(j)}\varepsilon_{t-k2^j}^{(j)}.
$$

that is a (type of) Wold representation describing any weakly-stationary time series of interest as a linear combination of shocks classified on the basis of their arrival time as well as their scale.

It is now instresting to discuss the relation between the coefficients $\psi_k^{(j)}$ of the extended Wold representation and the coefficents $\alpha_k$ of the classical Wold representation. Exploiting above equation after expressing $\epsilon_t^{j}$ as a finite linear combination of variables $\epsilon_t$ as in Eq.(5) and Eq.(4), we obtain

$$
\psi_{k}^{(j)}=\frac{1}{\sqrt{2^{j}}}\left(\sum_{i=0}^{2^{j-1}-1}\alpha_{k2^{j}+i}-\sum_{i=0}^{2^{j-1}-1}\alpha_{k2^{j}+2^{j-1}+i}\right).
$$


> [!TIP|label:Derivation]
> Substitute the Classical Wold Representation into the Projection Formula
$$
\psi_k^{(j)}=\mathbb{E}\left[\left(\sum_{m=0}^{+\infty}\alpha_m\varepsilon_{t-m}\right)\varepsilon_{t-k2^j}^{(j)}\right]
$$
> the definition of $\varepsilon_{t-k2^j}^{(j)}$ is
$$
\varepsilon_{t-k2^j}^{(j)}=\frac{1}{\sqrt{2^j}}\left(\sum_{i=0}^{2^{j-1}-1}\varepsilon_{t-k2^j-i}-\sum_{i=0}^{2^{j-1}-1}\varepsilon_{t-k2^j-2^{j-1}-i}\right)
$$
> plug the expression for $\varepsilon_{t-k2^j}^{(j)}$ into $\psi_k^{(j)}$ 
$$
\psi_k^{(j)}=\frac{1}{\sqrt{2^j}}\mathbb{E}\left[\left(\sum_{m=0}^{+\infty}\alpha_m\varepsilon_{t-m}\right)\left(\sum_{i=0}^{2^{j-1}-1}\varepsilon_{t-k2^j-i}-\sum_{i=0}^{2^{j-1}-1}\varepsilon_{t-k2^j-2^{j-1}-i}\right)\right]
$$
> Given the above, the expectation $\mathbf{E}[\varepsilon_{t-m} \varepsilon_{t-k2^j - i}]$ is non-zero only-if $m = k2^j + i$. Similarly, $\mathbb{E}\left[\varepsilon_{t-m}\varepsilon_{t-k2^j-2^{j-1}-i}\right]$ is non-zero only if $m = k2^j+2^{j-1}+i$. Thus, the sums simplifyu as:
$$
\psi_k^{(j)}=\frac{1}{\sqrt{2^j}}\left(\sum_{i=0}^{2^{j-1}-1}\alpha_{k2^j+i}-\sum_{i=0}^{2^{j-1}-1}\alpha_{k2^j+2^{j-1}+i}\right)
$$

Just like the shock $\mathbf{\varepsilon}^{j}$ are DHT of the high-frequency shocks in the classical Wold, the coefficients in the above equation are DHTs of the coefficients in the classical Wold. **Because the Wold coeffcients $\alpha_k$ are unique, it follows that the coefficients $\psi_k^{(j)}$ are also unique, and time-invairant, functions of the coefficents $\alpha_k$, given the Haar structure.**

### Multi-variate case

Consider the zero mean, covariance stationary, bi-variate process $\boldsymbol{X}=\{(y_t,x_t)^\intercal\}_{t\in\mathbb{Z}}$. Its Wold representation is, again,

$$
\boldsymbol{X}_{t}=\begin{pmatrix}y_{t}\\x_{t}\end{pmatrix}=\sum_{k=0}^{+\infty}\boldsymbol{\alpha}_{k}\boldsymbol{\varepsilon}_{t-k},
$$

where $\boldsymbol{\varepsilon}=\{(u_t,e_t)^{\mathsf{T}}\}_{t\in\mathbb{Z}}$ is a bi-variate vector of (possibly) cross-correlated white noise shocks and the Wild coefficients $\mathbf{\alpha}_k$ are, for all $k \in \mathbb{N}_0, 2 \times 2$ matrics.

By analogy with the scalar case, we can now write

$$
\begin{pmatrix}y_{t}\\x_{t}\end{pmatrix}=\sum_{j=1}^{+\infty}\sum_{k=0}^{+\infty}\Psi_{k}^{(j)}\varepsilon_{t-k2^{j}}^{(j)}.
$$

For any $j\in\mathbb{N}$, the $2\times 2$ matrices $\psi_k^{(j)}$ are unique DHTs of the original Wold coefficients:

$$
\boldsymbol{\Psi}_k^{(j)}=\frac{1}{\sqrt{2^j}}\left(\sum_{i=0}^{2^{j-1}-1}\boldsymbol{\alpha}_{k2^j+i}-\sum_{i=0}^{2^{j-1}-1}\boldsymbol{\alpha}_{k2^j+2^{j-1}+i}\right).
$$

Similarly,

$$
\boldsymbol{\varepsilon}_{t}^{(j)}=\frac{1}{\sqrt{2^{j}}}\left(\sum_{i=0}^{2^{j-1}-1}\boldsymbol{\varepsilon}_{t-i}-\sum_{i=0}^{2^{j-1}-1}\boldsymbol{\varepsilon}_{t-2^{j-1}-i}\right).
$$

Everything is the **same** as in the scalar process scenario.

## Spectral factor models

The methodology in this article is above. Then we go further to the identification. To operationalize the extended Wold representation, **we first need to compute the Wold coefficients $\mathbf{\alpha}_k$.** 

We write the vector time series of interest as $\mathbf{x}_t=\left(y_t,\widetilde{\mathbf{x}}_t^\intercal\right)^\intercal\in\mathbb{R}^k,$, where $y_t \in \mathbb{R}$ and $\tilde{\mathbf{x}} \in \mathbb{R}^{k-1}$. We assume $\mathbf{x}_t$ follows a linear vector autoregressive (VAR) process of order p of the form

$$
\mathbf{x}_t=A_1\mathbf{x}_{t-1}+\ldots+A_p\mathbf{x}_{t-p}+\boldsymbol{\varepsilon}_t,
$$

where the $A_i$s, with $i = 1,\cdots ,p$, are $k \times k$ parameter matrices and the error process, $\varepsilon_t$, is a $k$-dimensional zero-mean white noise process with covariance matrix $\mathbb{E}(\boldsymbol{\varepsilon}_t\boldsymbol{\varepsilon}_t^\intercal)=\Sigma_\varepsilon.$

As is well known, the VAR(p) process can be written as a $kp$-dimensional VAR(1) process by stacking p consecutive $\mathbf{x}_t$ variables in a $(kp \times 1)$-dimensional vector $\mathbf{X}_t=\left(\mathbf{x}_t^\intercal,\mathbf{x}_{t-1}^\intercal,\ldots,\mathbf{x}_{t-p+1}^\intercal\right)^\intercal.$ Thus, we have 

$$
\mathbf{X}_t=A\mathbf{X}_{t-1}+U_t,
$$

where the matrix $A$ is sometimes referred to as the "**companion matrix**" of the VAR(p) process.

> [!TIP|label:Companion matrix]
> The lag is $p$ and the number of vairables is $m$, then the companion matrix looks like 
$$
\left.\begin{array}{ccccc}A_{mp\times mp}&=&\left(\begin{array}{cccc}A_1&A_2&\cdots&A_{p-1}&A_p\\I_m&0&\cdots&0&0\\0&I_m&\ddots&\vdots&0\\\vdots&\ddots&\ddots&0&\vdots\\0&\cdots&0&I_m&0\end{array}\right.\end{array}\right)
$$
> then the companion form of VAR is 
$$
\begin{aligned}\tilde{Y}_{t}&=\quad\begin{pmatrix}Y_t\\Y_{t-1}\\\vdots\\Y_{t-p+1}\end{pmatrix}\\&=\quad\begin{pmatrix}A_1Y_{t-1}+\cdots+A_pY_{t-p}\\Y_{t-1}\\\vdots\\Y_{t-p+1}\end{pmatrix}+\begin{pmatrix}\varepsilon_t\\0\\\vdots\\0\end{pmatrix}\end{aligned}
$$
> The power $k$ of companion matrix actually **encapsulate the cumulative effect of past innovations**. For example, when $K=3,p=2$, the companion matrix is
$$
M = \begin{bmatrix}A_{1}&A_{2}\\I_{3}&0_{3\times3}\end{bmatrix}
$$
> then 
$$
M^2=M\times M=\begin{bmatrix}A_1^2+A_2I_3&A_1A_2\\A_1&A_2\end{bmatrix}
$$
> **So actually, we only need the top-left part of the matrix.** It is because 
$$
\begin{aligned}
x_t &= A_1 x_{t-1} + A_2 x_{t-2} \\
&= A_1^2 x_{t-2} + A_1 \varepsilon_{t-1} + A_2 x_{t-2} + \cdots \\
&= A_1 \varepsilon_{t-1} + (A_1^2 + A_2)\varepsilon_{t-2} + \cdots
\end{aligned}
$$
> 

Under standard assumptions, a stable VAR has a timeinvariant mean and a time-invariant variance/covariance matrix and is therefore covariance stationary. Put differently, **the process admits a Wold representation.** 

> [!TIP]
> The lag of VAR does not affect the lag of Wold representation. For example a simple univariate **AR(1)** model will have infinite sum of $\varepsilon$.
> 
> Transform to VAR1 to use the recursive property
$$
\mathbf{X}_t=A(A\mathbf{X}_{t-2}+U_{t-1})+U_t=A^2\mathbf{X}_{t-2}+AU_{t-1}+U_t
$$
> repeating this process recursively leads to 
$$
\mathbf{X}_t=\sum_{k=0}^\infty A^kU_{t-k}
$$
> 

Indeed, the stability of the process ensures the existence of the inverse VAR operator $(I_{kp}-AL)^{-1}=\sum_{k=0}^\infty\boldsymbol{\alpha}_kL^k$, where $\alpha_k=A^k$. As a result, one can obtain the following Wold moving average representation of $\mathbf{X}_t$:

$$
\mathbf{X}_t=\sum_{k=0}^\infty\alpha_kU_{t-k}.
$$

We can compute Wold coefficients using companion matrix and then get the extended wold coefficients. As for $\varepsilon^{(j)}$, we will use the $U_t$ which is get from the VAR1 to calculate.

### Incorporating economic restrictions: identification in practice

In practice, **we account for predictability in market returns**. We do so by setting the vector $\tilde{\mathbf{x}}_t$ as being the market excess return followed by three state variables:

- the yield spread between long-term adn short-term bonds (the difference between the ten-year constant maturity bond yield and the yield on short-term notes)
- the market's price dividend ratio (logarithmic ratio of the CRSP price index and a one-year moving average of dividends)
- small-stock value spread (difference between the logarithmic book-to-market ratio of small value and small growth stocks)

More specifically, for each asset's excess return $y$, we estimate a VAR(p) with block exogeneity defined as follows:

$$
\begin{aligned}
y_{t}&=a_{y}+A_{1,y}Y_{t-1}+A_{2,y}\circ\tilde{\mathbf{X}}_{t-1}+\varepsilon_{t}^{1} \\
\widetilde{\mathbf{x}}_{t}&=a_{x}+A_{1,x}Y_{t-1}+A_{2,x}\circ\widetilde{\mathbf{X}}_{t-1}+\boldsymbol{\varepsilon}_{t}^{2},
\end{aligned}
$$

Importantly, we set $A_{1,x}=0$, which means $y$ does not Granger cause $\tilde{\mathbf{x}}$.



