# Frequency in High Dimension

## Motivation

首先计算收益率的协方差并将协方差分解到不同频率上，对不同频率的协方差进行 PCA 得到

$$
\Sigma_L = Q_L \Lambda_L Q_L^T , \quad \Sigma_H = Q_H \Lambda_H Q_H^T
$$

接着构造频率因子

$$
F_L = Q_L^T R, \quad F_H = Q_H^T R
$$

接着发现 $F_L$ 并不完全是低频信息，这是因为 $R$ 中包含了 $R_L,R_H$，并且 $Q_L^T R_H \neq 0$。

不过，按理说，这是由于 $Q_L,Q_H$ 作为不同频率协方差分解出的主成分并非正交引起的。不同频率的收益率之间是正交的，但是分解出的主成分特征向量矩阵却不是正交的。

并且，在尝试寻找 $Q_H$ 正交补的过程中，发现 $Q_H$ 是满秩的，这是因为 $Q_H$ 的维度为 $N \times N$，而频率（高频）的数量（与 $T$ 有关）远大于 $N$，也就是说，**用过多的频率表示少数的资产**，所以最终是满秩的。

在 $N$ 维空间内，$Q_H$ 已经是满秩了，那么 $Q_L$ 处于这个空间，二者的信息就一定有重叠之处，不会正交。但如果能够将高频特征向量和低频特征向量**投影到更高的维度**，也许就可以在更高的维度内实现二者的正交。

至于所投影的这个高维空间，可以称之为，**频率特征向量空间**，不同频率的特征向量，本该是正交的。

## Methodology

Kernel method：将数据投影到高维空间，用线性关系将数据分开。低维中的非线性问题可以转化为高维空间内的线性问题

Feature mapping $\phi : \ \mathbb{R}^m \ \rightarrow \mathbb{R}^n$

> [!TIP|label:Example]
> 例如 $\phi : (x_1,x_2) \rightarrow (z_1,z_2,z_3) = (x_1^2,\sqrt{2}x_1x_2,x_2^2) $
> 那么原坐标空间中的两点内积就为
$$
\begin{aligned}
<\phi(x_1,x_2),\phi(x_1',x_2')>&=<(z_1,z_2,z_3),(z_1',z_2',z_3')> \\
&=<(x_{1}^{2},\sqrt{2}x_{1}x_{2},x_{2}^{\prime2}),(x_{1}^{\prime2},\sqrt{2}x_{1}^{\prime}x_{2}^{\prime},x_{2}^{\prime2})> \\
&=x_{1}^{2}x_{1}^{\prime2}+2x_{1}x_{2}x_{1}^{\prime}x_{2}^{\prime}+x_{2}^{2}x_{2}^{\prime2}\\
&= (x_1x_1^{\prime}+x_2x_2^{\prime})^2 = (<x,x'>)^2 = \kappa(x,x') \quad \text{Kernel Function}
\end{aligned}
$$
> 这意味着，**高维度空间两点的内积等于原空间维度两点内积的平方**，这就是 Kernel function，高维度空间中两点的内积。

**Distance in the Feature Space**

高维度空间中两点的**距离**

$$
\begin{aligned}
\left\|\phi(x)-\phi(x')\right\|^2&=\left(\phi(x)-\phi(x')\right)^T\left(\phi(x)-\phi(x')\right) \\
&=\phi(x)^T\phi(x)-2\phi(x)^T\phi(x^\prime)+\phi(x^\prime)^T\phi(x^\prime) \\
&=<\phi(x),\phi(x)>-2<\phi(x),\phi(x^{\prime})>+<\phi(x^{\prime}),\phi(x^{\prime})> \\
&= \kappa(x,x) - 2 \kappa (x,x') + \kappa (x',x')
\end{aligned}
$$

高维度空间中两个点的**角度**

$$
\begin{aligned}
    &<\phi(x),\phi(x')>=\parallel\phi(x)\parallel\cdot\parallel\phi(x')\parallel\cos\theta  \\
    \Longrightarrow & \cos\theta=\frac{<\phi(x),\phi(x^{\prime})>}{\parallel\phi(x)\parallel\cdot\parallel\phi(x^{\prime})\parallel}=\frac{<\phi(x),\phi(x^{\prime})>}{\sqrt{<\phi(x),\phi(x)>}\sqrt{<\phi(x^{\prime}),\phi(x^{\prime})>}} \\
    &= \frac{\kappa(x,x')}{\sqrt{\kappa(x,x)}\sqrt{\kappa(x',x')}}
\end{aligned}
$$

根据这几条性质可以发现，$\phi$ 长什么样并不重要，只需要知道内积就可以了，说明了内积的重要性，所以有内积矩阵这个概念

> [!Warning]
> 这只是其中一个 kernel function，性质并不能完全移植到别的 kernel 上去。

$$
\begin{aligned}
\text{K}& =\begin{bmatrix}<\phi(x_{1}),\phi(x_{1})>&\cdots&<\phi(x_{1}),\phi(x_{N})>\\\vdots&\ddots&\vdots\\<\phi(x_{N}),\phi(x_{1})>&\cdots&<\phi(x_{N}),\phi(x_{N})>\end{bmatrix}  \\
&=\begin{bmatrix}\kappa(x_1,x_1)&\cdots&\kappa(x_1,x_N)\\\vdots&\ddots&\vdots\\\kappa(x_N,x_1)&\cdots&\kappa(x_N,x_N)\end{bmatrix}
\end{aligned}
$$

那么什么样的 $\kappa$ 才能用呢？

- **Finitely positive semi-definite**

有限半正定指的是，给定任意有限点 $\{x_1,\cdots,x_n\}$，都可以算出 Kernel matrix，这个 Kernel matrix 都需要是半正定的。

**Characterization of Kernels**

如果 Kernel function 满足 finitely positive semi-definite，就可以找到一个对应的 $\phi$，但不止一个。反过来如果能够找到一个 $\phi$，也可以找到对应的 kernel function。

> [!NOTE|label:Widely used kernel functions]
> Linear Kernel
$$
\kappa(x,z)=<x,z>
$$
> Polynomial Kernel
$$
\kappa(x,z)=(<x,z>+1)^r,r\in Z^+
$$
> RBF (Gaussian) Kernel
$$
\kappa(x,z)=\exp\biggl(\frac{-\left\|x-z\right\|^2}{2\sigma^2}\biggr),\sigma\in R-\{0\}
$$
> 

目前的问题就是，如何找到对应的 Kernel？而且对 kernel 多了一些了解之后，感觉这样似乎达不到预期的效果。

关于这一点，要想一想，升维的目的是什么？因为我假设存在**频率收益率因子**，每一个频率都对应一个频率收益率因子，每个资产收益率，是频率收益率因子根据不同权重组合而成的，所以为了得到频率收益率因子，就有升维到更高空间的必要。

不过，对于收益率进行傅里叶分解，可以得到振幅系数和 $\cos$ 的组合，此时振幅系数已经可以作为权重，但 $\cos$ 好像又不能代表收益率，这也得再想想。



