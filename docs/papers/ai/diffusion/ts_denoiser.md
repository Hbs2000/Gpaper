# A Financial Time Series Denoiser Based on Diffusion Models

Zhuohan Wang<sup>1</sup>, Carmine Ventre<sup>1</sup>, *ICAIF*, 2024.

1. King's College London

Use diffusion model as a denoiser for financial time series in order to improve data predictability and trading performance. By leveraging the forward and reverse processes of a conditional diffusion model to add an remove noise progressively, we *reconstuct* original data from noisy inputs.

## Related works

### Diffusion Model Theory

There are two threads of diffusion models, which are eventually unified. The first thread of the diffusion model is inspired by **thermodynamics**, laying the groundwork for diffusion models by proposing the idea of training generative models through gradual noise addition and removal.

> Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. 2015. Deep unsupervised learning using nonequilibrium thermodynamics. In *International conference on machine learning*. PMLR, 2256–2265.

Ho et al. Proposed the **Denoising Diffusion Probabilisitic Model** (DDPM) in 2020, marking a significant milestone for diffusion models. Their mothod involves adding noise in a forward process and denoising in a reverse process to generate high-quality images. Subsequent work focuses on how to improve sampling quality and speed.

> Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. *Advances in neural information processing systems* 33 (2020), 6840–6851.
>
> Alexander Quinn Nichol and Prafulla Dhariwal. 2021. Improved denoising diffusion probabilistic models. In **International conference on machine learning**. PMLR, 8162–8171.
>
> Jiaming Song, Chenlin Meng, and Stefano Ermon. 2020. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502 (2020).

The second thread of diffusion models originated from **Score-based Generative Models** (SGM) by using Denoising Score Matching (DSM). It involves training a model to predict the gradient of the data density (score) by minimizing the denoising score matching loss, which can be used for reconstructing clean data from noisy observations. Song and Ermon showed that once the score function is learned, samples can be generated by iteratively refining noisy samples using **Langevin dynamics**.

> Yang Song and Stefano Ermon. 2019. Generative modeling by estimating gradients of the data distribution. *Advances in neural information processing systems* 32 (2019).
>
> Pascal Vincent. 2011. A connection between score matching and denoising autoencoders. *Neural computation* 23, 7 (2011), 1661–1674.

The above DDPM and SGM can be unified under the **Stochastic Differential Equation** (SDE) framework. This framework allows both training and sampling processes to be formulated using SDEs, providing a more elegant and theoretically grounded approach to diffusion modeling, providing a more elegant and theoretically grounded approach to diffusion modeling. Furthermore, Karras et al. disentangle the complex design space of the diffusion model in the context of DSM, enabling significant improvements in sampling quality, training costs, and generation speed.

> Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. 2020. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456 (2020).
>
> Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. 2022. Elucidating the design space of diffusion-based generative models. *Advances in Neural Information Processing Systems* 35 (2022), 26565–26577.


### Diffusion Model on Time Series

The applications of diffusion models on time series can be divided into three categories, which are **forecasting, imputation, and generation**. TimeGrad is an auto-regressive time series for forecasting model based on DDPM, while ScoreGrad is developed on the SDE framework. For time series imputation task, CSDI uses transformer as backbone model architecture, and SSSD uses structured state space for sequence modeling method. For the time series generation task, DiffTime and its variants are proposed for constrained time series generation. D3VAE uses a coupled diffusion process for multivariate time series augmentation, then uses a bidirectional VAE with denoising score matching to clear the noise.

> Lequan Lin, Zhengkun Li, Ruikun Li, Xuliang Li, and Junbin Gao. 2023. Diffusion models for time-series applications: a survey. *Frontiers of Information Technology & Electronic Engineering* (2023), 1–23.
>
> Kashif Rasul, Calvin Seward, Ingmar Schuster, and Roland Vollgraf. 2021. Autoregressive denoising diffusion models for multivariate probabilistic time series forecasting. *In International Conference on Machine Learning*. PMLR, 8857–8868.
>
> Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. 2021. Csdi: Conditional score-based diffusion models for probabilistic time series imputation. Advances in Neural Information Processing Systems 34 (2021), 24804–24816.
>
> Tijin Yan, Hongwei Zhang, Tong Zhou, Yufeng Zhan, and Yuanqing Xia. 2021. Scoregrad: Multivariate probabilistic time series forecasting with continuous energy-based generative models. arXiv preprint arXiv:2106.10121 (2021).
>
> Juan Miguel Lopez Alcaraz and Nils Strodthoff. 2022. Diffusion-based time series imputation and forecasting with structured state space models. arXiv preprint arXiv:2208.09399 (2022).
>
> Andrea Coletta, Sriram Gopalakrishnan, Daniel Borrajo, and Svitlana Vyetrenko. 2024. On the constrained time-series generation problem. *Advances in Neural Information Processing Systems* 36 (2024).
>
> Yan Li, Xinjiang Lu, Yaqing Wang, and Dejing Dou. 2022. Generative time series forecasting with diffusion, denoise, and disentanglement. *Advances in Neural Information Processing Systems* 35 (2022), 23009–23022.


### Denoising Financial Time Series

In Song et al., padding-based Fourier transform is proposed to eliminate the noise waveform in the frequency domain of financial time series data and recurrent neural networks are used to forecast future stock market index. A hybrid model comprising of wavelet transform and optimized extreme learning machine is proposed in Yu et al to present a stable and precise prediction of financial time series. Ma et al try to denoise financial time series and demonstrate that denoised labels improve the performances of the downstream learning algorithm.

> Donghwan Song, Adrian Matias Chung Baek, and Namhun Kim. 2021. Forecasting stock market indices using padding-based fourier transform denoising and time series deep learning models. *IEEE Access* 9 (2021), 83786–83796.
>
> He Yu, Li Jing Ming, Ruan Sumei, and Zhao Shuping. 2020. A hybrid model for financial time series forecasting—integration of EWT, ARIMA with the improved ABC optimized ELM. *IEEE Access 8* (2020), 84501–84518.
>
> Yanqing Ma, Carmine Ventre, and Maria Polukarov. 2022. Denoised Labels for Financial Time Series Data via Self-Supervised Learning. In Proceedings of the Third ACM *International Conference on AI in Finance*. 471–479.

## Score Based Generative Model: Background

What is score? Score is the log gradient of probability density function of data

$$
\begin{equation}
    \text{score} = \nabla_x \log p_{\text{data}}(x)
\end{equation}
$$

Score 能告诉我们的是，当我站在数据空间中的点 $x$，应该朝哪个方向移动一点点，才能让数据的概率密度 $p_{\text{data}}(x)$ 上升的最快。

我们可以直接使用神经网络 $s_{\theta}$ 来拟合score，优化目标如下

$$
\begin{aligned}
\mathcal{L}(\theta)&=\frac{1}{2}\mathbb{E}_{p_{data}(x)}\left[||\mathbf{s}_\theta(\mathbf{x})-\nabla_\mathbf{x}\log p_{data}(\mathbf{x})||_2^2\right]\\
&= \|\mathbf{s}_\theta\|_2^2-2\mathbf{s}_\theta^T(\nabla\log p_\mathrm{data})+\|\nabla\log p_\mathrm{data}\|_2^2 \\
&= \mathbb{E}\left[\frac{1}{2}\|\mathbf{s}_\theta\|_2^2\right]-\mathbb{E}\left[\mathbf{s}_\theta^T(\nabla\log p_\mathrm{data})\right]+\mathrm{const} \\
&=\mathbb{E}_{p_{data}(x)}\left[tr(\nabla_{\mathbf{x}}\mathbf{s}_\theta(\mathbf{x}))+\frac{1}{2}||\mathbf{s}_\theta(\mathbf{x})||_2^2\right]+\mathrm{const}.
\end{aligned}
$$

其中第二项好计算，就是神经网络的输出，但是第一部分是 Jacobian matrix 的迹，需要对所有的 $x$ 计算二阶导数。

这里就需要用到 DSM 了，通过给 $x$ 加入一个高斯噪音，这里可以直接拟合加噪后的 Score

$$
\begin{equation}
\mathcal{L}_{\mathrm{DSM}}(\theta)=\frac{1}{2}\mathbb{E}_{q_\sigma(\tilde{\mathbf{x}}|\mathbf{x})p_{\mathrm{data}}(\mathbf{x})}\left[\|\mathbf{s}_\theta(\tilde{\mathbf{x}})-\nabla_{\tilde{\mathbf{x}}}\log q_\sigma(\tilde{\mathbf{x}}|\mathbf{x})\|_2^2\right]
\end{equation}
$$

此时期望中是条件分布 $q_\sigma(\tilde{\mathbf{x}}|\mathbf{x})$ 而非 $q_\sigma(\tilde{\mathbf{x}})$，通过人为设计的加噪过程

$$
\tilde{\mathbf{x}} = \mathbf{x} + \sigma \epsilon, \quad \epsilon \sim \mathcal{N}(0,I)
$$

其条件分数是**解析可知**的，例如高斯噪声

$$
\nabla_{\tilde{\mathbf{x}}}\log q_\sigma(\tilde{\mathbf{x}}|\mathbf{x})=-\frac{\tilde{\mathbf{x}}-\mathbf{x}}{\sigma^2}
$$

其物理意义就是从噪点 $\tilde{\mathbf{x}}$ 指向原始点 $\mathbf{x}$ 的向量，缩放因子($1/\sigma^2$)。**此时 label 是完全可求的，所以能够直接训练**。

通过最小化上述目标，实际上是在训练 $s_{\theta}(\tilde{x})$ 逼近加噪数据的边缘分布 $q_{\sigma}(\tilde{\mathbf{x}})$ 的分数，即

$$
\begin{equation}
\mathbf{s}_\theta(\tilde{\mathbf{x}})\approx\nabla_{\tilde{\mathbf{x}}}\log q_\sigma(\tilde{\mathbf{x}})
\end{equation}
$$

根据物理意义，短期来看，DSM 的训练过程是令加噪 $\tilde{\mathbf{x}}$ 指向具体样本点 $\mathbf{x}$，长期来看，根据 vincent (2011)，

$$
\mathbb{E}_{x\sim p_{\mathrm{data}}}\left[\|s_\theta(\tilde{x})-\nabla_{\tilde{x}}\log q_\sigma(\tilde{x})\|_2^2\right]=\mathcal{L}_{\mathrm{DSM}}+\text{const}
$$

当 $\sigma \rightarrow 0$ 时，$q_{\sigma}(\tilde{x})$ 逼近于 $p_{\mathrm{data}}(x)$，因此当网络容量足够大且噪声 $\sigma$ 较小，拟合 $q_{\sigma}(\tilde{\mathbf{x}})$ 的f分数价于逼近原始 $p_{\mathrm{data}}(\mathbf{x})$ 的分数结构。

> Pascal Vincent. 2011. A connection between score matching and denoising autoencoders. *Neural computation* 23, 7 (2011), 1661–1674.


### Score matching with Langevin Dynamics

> SGM 的采样方法是通过朗之万动力学来进行连续迭代。

### Denoising Diffusion Probabilistic Model

DDPM can be seen as a **hierarchical markovian variational autoencoder**. 

**正向过程**

给定 noise scheduler $0<\beta_1<\beta_2<\cdots<\beta_N<1$，定义 Markov chain

$$
\begin{equation}
p(\mathbf{x}_{\boldsymbol{i}}|\mathbf{x}_{\boldsymbol{i}-1})=\mathcal{N}(\mathbf{x}_{\boldsymbol{i}}|\sqrt{1-\beta_{\boldsymbol{i}}}\mathbf{x}_{\boldsymbol{i}-1},\beta_{\boldsymbol{i}}\mathbf{I}).
\end{equation}
$$

> 这里不同于 SGM 的单步加噪，而是定义了多步马尔可夫链的固定调度。

**单步重参数化**

$$
\begin{equation}
\mathbf{x}_i=\sqrt{1-\beta_i}\mathbf{x}_{i-1}+\sqrt{\beta_i}\boldsymbol{\epsilon}_i,\quad\boldsymbol{\epsilon}_i\sim\mathcal{N}(0,\mathbf{I})
\end{equation}
$$

通过递归展开，可直接从 $\mathbf{x}_0$ 计算任意 $\mathbf{x}_i$：

$$
\begin{equation}
\mathbf{x}_i=\sqrt{\alpha_i}\mathbf{x}_0+\sqrt{1-\alpha_i}\boldsymbol{\epsilon},\quad\boldsymbol{\epsilon}\sim\mathcal{N}(0,\mathbf{I})
\end{equation}
$$

其中 $\alpha_i = \prod_{j=1}^i(1-\beta_j)$，对应边缘分布

$$
\begin{equation}
p_{\alpha_i}(\mathbf{x}_i|\mathbf{x}_0)=\mathcal{N}\left(\mathbf{x}_i\mid\sqrt{\alpha_i}\mathbf{x}_0,(1-\alpha_i)\mathbf{I}\right)
\end{equation}
$$

令 $\alpha_N \sim 0$，使得最终分布 $p_{\alpha_N}(\mathbf{x}_N)\approx\mathcal{N}(0,\mathbf{I})$.

**反向过程**

定义参数化的去噪转移

$$
\begin{equation}
p_\theta(\mathbf{x}_{i-1}|\mathbf{x}_i)=\mathcal{N}\left(\mathbf{x}_{i-1};\frac{1}{\sqrt{1-\beta_i}}\left(\mathbf{x}_i+\beta_is_\theta(\mathbf{x}_i,i)\right),\beta_i\mathbf{I}\right)
\end{equation}
$$

将反向公式改写

$$
\begin{equation}
\mathbf{x}_{i-1}=\frac{\mathbf{x}_i}{\sqrt{1-\beta_i}}+\frac{\beta_i}{\sqrt{1-\beta_i}}s_\theta(\mathbf{x}_i,i)+\sqrt{\beta_i}\mathbf{z}_i
\end{equation}
$$

公式与朗之万动力学**统一**。

DDPM 的目标函数是加权证据下界和 (weighted sum of evidence lower bound)，最小化

$$
\begin{equation}
\sum_{i=1}^N(1-\alpha_i)\mathbb{E}_{p_{data}(\mathbf{x})}\mathbb{E}_{p_{\alpha_i}(\tilde{\mathbf{x}}|\mathbf{x})}\left[||\mathbf{s}_{\theta^*}(\tilde{\mathbf{x}},i)-\nabla_{\tilde{\mathbf{x}}}\log p_{\alpha_i}(\tilde{\mathbf{x}}|\mathbf{x})||_2^2\right].
\end{equation}
$$

其中

$$
\nabla_{\mathbf{x}_i}\log p_{\alpha_i}(\mathbf{x}_i|\mathbf{x}_0)=-\frac{\mathbf{x}_i-\sqrt{\alpha_i}\mathbf{x}_0}{1-\alpha_i}
$$

代入得

$$
p_{\alpha_i}(\mathbf{x}_i|\mathbf{x}_0)\propto\exp\left(-\frac{\|\mathbf{x}_i-\sqrt{\alpha_i}\mathbf{x}_0\|^2}{2(1-\alpha_i)}\right)
$$

因此梯度计算有

$$
\nabla_{\mathbf{x}_i}\log p_{\alpha_i}=-\frac{\mathbf{x}_i-\sqrt{\alpha_i}\mathbf{x}_0}{1-\alpha_i}=-\frac{\epsilon}{\sqrt{1-\alpha_i}}
$$

因此损失函数简化为噪声预测

$$
\begin{equation}
\mathbb{E}\left[\|s_\theta(\mathbf{x}_i,i)+\frac{\epsilon}{\sqrt{1-\alpha_i}}\|_2^2\right]
\end{equation}
$$

也就是说，对任意给定的噪声等级 $i$ 和加噪样本 $\tilde{\mathbf{x}}$，存在解析解

$$
\begin{equation}
\nabla_{\tilde{x}}\log p_{\alpha_i}(\tilde{x})\propto-(\tilde{x}-\mathbb{E}[x|\tilde{x}])
\end{equation}
$$

噪声是从原始数据指向含噪数据，而 score function 是从含噪数据指向原始数据，**二者互为反向**。

> Calvin Luo. 2022. Understanding diffusion models: A unified perspective. arXiv preprint arXiv:2208.11970 (2022).

> **DDPM 是 SGM 在离散马尔科夫链设定下的特例，通过变分推断导出了显示优化目标，而 SGM 提供了更灵活的连续框架。**
>
> 此外，不同于 SGM 的朗之万动力学连续采样，DDPM 使用 ancestral sampling 进行离散步骤采样。

### Stochastic Differential Equation: A Unified Perspective

Song et al.(2020) 指出 SMLD 和 DDPM 可以在 SDE 的框架下统一。令 $\{\mathbf{x}(t)\}_{t=0}^T$ 表示为一个随机的 diffusion 过程，并且时间连续 $t \in [0,T]$。$p_0$ 是真实数据分布 $p_T$ 是可得先验分布如 $\mathbf{x} \sim p_0, \mathbf{x}_T \sim p_T$，并且定义从 $\mathbf{x}(s)$ 到 $\mathbf{x}(p)$ 的 transition kernel 为 $p_{st}(\mathbf{x}(t)|\mathbf{x}(s))$，其中 $0\leq s < t \leq T$。

因此可用一个 SDE 来表示这一前向过程

$$
\begin{equation}
d\mathbf{x}=\mathbf{f}(\mathbf{x},t)\mathrm{~}dt+g(t)\mathrm{~}d\mathbf{w}
\end{equation}
$$

其中 $f(x,t)$ 是 drift term，$g(t)dw$ 是 diffusion term。$w$ 是 standard Wiener process $dw \sim \mathcal{N}(0,\mathbf{I})$.

这一 SDE 的反向过程也是一个 SDE，

$$
\begin{equation}
d\mathbf{x}=\left[\mathbf{f}(\mathbf{x},t)-g^2(t)\nabla_\mathbf{x}\log p_t(\mathbf{x})\right]dt+g(t)d\bar{\mathbf{w}}
\end{equation}
$$

其中 $\bar{\mathbf{w}}$ 是一个反向 Wiener process，$\nabla_\mathbf{x}\log p_t(\mathbf{x})$ 是每个 $t$ 时间点的边缘分布 score。

**SMLD**

SMLD 的离散正向过程如下，

$$
\begin{equation}
\mathbf{x}_i=\mathbf{x}_{i-1}+\sqrt{\sigma_i^2-\sigma_{i-1}^2}\cdot\mathbf{z}_{i-1},\quad\mathbf{z}_{i-1}\sim\mathcal{N}(0,I)
\end{equation}
$$

其中 $\sigma_i > \sigma_{i-1}$ 是递增的噪声参数。

定义时间刻度 $t_i = i/N$，令 $\sigma(t)$ 为平滑函数满足 $\sigma(t_i)=\sigma_i$。当 $N \rightarrow \infty$ 时有，

$$
\Delta\mathbf{x}=\mathbf{x}_i-\mathbf{x}_{i-1}=\sqrt{\sigma^2(t_i)-\sigma^2(t_{i-1})}\cdot\mathbf{z}_{i-1}
$$

利用微分近似：

$$
\sigma^2(t_i)-\sigma^2(t_{i-1})\approx\frac{d\sigma^2(t)}{dt}\Delta t,\quad\Delta t=t_i-t_{i-1}=\frac{1}{N}
$$

且 $\mathbf{z}_{i-1}\approx\frac{\Delta\mathbf{w}}{\sqrt{\Delta t}}$，代入得

$$
\Delta\mathbf{x}\approx\sqrt{\frac{d\sigma^2(t)}{dt}\Delta t}\cdot\frac{\Delta\mathbf{w}}{\sqrt{\Delta t}}=\sqrt{\frac{d\sigma^2(t)}{dt}}\Delta\mathbf{w}
$$

取极限 $\Delta t \rightarrow 0$ 即得 Variance Exploding (VE) SDE

$$
\begin{equation}
d\mathbf{x}=\sqrt{\frac{d\sigma^2(t)}{dt}}d\mathbf{w}
\end{equation}
$$

> 方差变化率恒为正，意为方差随时间单调递增。

**DDPM**

DDPM 的离散正向扩散，

$$
\mathbf{x}_i=\sqrt{1-\beta_i}\mathbf{x}_{i-1}+\sqrt{\beta_i}\mathbf{z}_{i-1},\quad\mathbf{z}_{i-1}\sim\mathcal{N}(0,I)
$$

定义 $\beta(t)$ 满足 $\beta_i = \beta(t_i)\Delta t $，其中 $\Delta t =t_i - t_{i-1} = 1/N $。

展开有

$$
\mathbf{x}(t_i)=\sqrt{1-\beta(t_i)\Delta t}\mathbf{x}(t_{i-1})+\sqrt{\beta(t_i)\Delta t}\mathbf{z}(t_{i-1})
$$

对 $\sqrt{1 - \beta(t)\Delta t}$ 进行一阶展开，

$$
\sqrt{1-\beta(t)\Delta t}=1-\frac{\beta(t)}{2}\Delta t+\mathcal{O}(\Delta t^2)
$$

状态增量为

$$
\begin{aligned}\Delta\mathbf{x}&=\mathbf{x}(t_i)-\mathbf{x}(t_{i-1})\\&=\left[1-\frac{\beta(t_i)}{2}\Delta t\right]\mathbf{x}(t_{i-1})+\sqrt{\beta(t_i)\Delta t}\mathbf{z}(t_{i-1})-\mathbf{x}(t_{i-1})\\&=-\frac{\beta(t_i)}{2}\Delta t\cdot\mathbf{x}(t_{i-1})+\sqrt{\beta(t_i)}\cdot\sqrt{\Delta t}\mathbf{z}(t_{i-1})\end{aligned}
$$

取极限令 $\Delta t \rightarrow 0$ 有

$$
\begin{equation}
d\mathbf{x}=-\frac{\beta(t)}{2}\mathbf{x}dt+\sqrt{\beta(t)}d\mathbf{w}
\end{equation}
$$

通常我们定义坐标原点为高概率密度区，因此这里的负系数会使得 $\mathbf{x}$ 的绝对值减小，提供恢复到高密度区域的拉力，最终会和 $dw$ 项形成一种动态平衡，而不会 variance explode，所以也称之为 Variance Preserving (VP) SDE。





